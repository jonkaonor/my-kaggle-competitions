{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jonathankao/dec-2023-tabular-ensemble-xgboost-lgbm?scriptVersionId=157352394\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"0e93534a","metadata":{"papermill":{"duration":0.010858,"end_time":"2024-01-01T22:38:31.033118","exception":false,"start_time":"2024-01-01T22:38:31.02226","status":"completed"},"tags":[]},"source":["# Task Overview \n","Task: In this synthetic dataset based off of a real dataset funded by the Mayo Clinic, each example represents both general and survival information about a patient that has liver cirrhosis, a condition involving prolonged liver damage. The goal is to train a machine learning model that can predict the patient's current survival status based on the data features. \n","\n","Approach v1.0: Our approach will be to train a Logistic Regression model using sklearn to be used as a baseilne model and to then train a performance-focused model using XGBoost as a learning exercise.\n","\n","Approach v2.0: After a few days of initial work, we realized that it was feasible within the competition timeframe to improve on Approach 1 by training an Ensemble of models so we changed our approach to training multiple Logistic Regression, Random Forest, and XGBoost models and then combining them into an ensemble.\n","\n","Approach v3.0: After manual hyperparameter tuning all the models, we realized that the current Logistic Regression and Random Forest models were too weak to be implemented in the final Ensemble model. Instead we decided to add a LGBM classifier and create an Ensemble model with a LGBM classifier + XGBoost classifier. \n","\n","Next Steps: Try to implement cross-validation to hyperparameter tuning to reduce variance in log-loss score in the validation set. Try to implement automated hyperparameter tuning so that we can improve and then add more models to the Ensemble. \n","\n","Version History\n","1. v1.0-1.3 - Implemented dataset loading and dataset pre-processing. Added explanations and comments for each step. \n","2. v1.4 - Added Logistic Regression model training and Kaggle submission formatting\n","3. v2.0 - Added Random Forest and XGBoost model training and prediction.\n","4. v2.1 - Added log-loss calculations for each model on the validation set. \n","5. v2.2 - Added initial hyperparameter tuning for Logistic Regression, Random Forest, and XGBoost models. \n","6. v3.0 - Added LGBM Classifier and did hyperparameter tuning for the LGBM Classifier. Added explanations for model training / tuning\n","7. v3.1 - Combined the tuned LGBM Classifier and XGBoost classifer into an Ensemble model. \n","8. v3.2 - Switched to K-Fold Cross Validation approach for all hyperparameter tuning. \n","9. v3.3 - Combined original dataset with the synthetic dataset for training and updated log-loss scores accordingly. \n","\n","Log-Loss Score Logs (Validation Scores using Synthetic Dataset only)\n","1. Default Logistic Regression / Tuned Logistic Regression - 0.525 / 0.525\n","2. Default Random Forest / Tuned Random Forest - 0.550 / 0.482\n","3. Default XGBoost / Tuned XGBoost - 0.515 / 0.437 \n","4. Default LGBM / Tuned LGBM - 0.472 / 0.425\n","5. Tuned XGBoost / LGBM Ensemble Model - 0.431\n","\n","Log-Loss Score Logs (Validation Scores using Both the Synthetic Dataset and Original Dataset)\n","1. Tuned Logistic Regression - 0.53\n","2. Tuned Random Forest - 0.507\n","3. Tuned XGBoost - 0.414\n","4. Tuned LGBM - 0.409\n","5. Tuned XGBoost / LGBM Ensemble Model - 0.409"]},{"cell_type":"markdown","id":"71f83046","metadata":{"papermill":{"duration":0.010216,"end_time":"2024-01-01T22:38:31.054464","exception":false,"start_time":"2024-01-01T22:38:31.044248","status":"completed"},"tags":[]},"source":["# Dataset Loading"]},{"cell_type":"code","execution_count":1,"id":"ac42338c","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.078403Z","iopub.status.busy":"2024-01-01T22:38:31.077652Z","iopub.status.idle":"2024-01-01T22:38:31.660797Z","shell.execute_reply":"2024-01-01T22:38:31.659525Z"},"papermill":{"duration":0.5981,"end_time":"2024-01-01T22:38:31.663357","exception":false,"start_time":"2024-01-01T22:38:31.065257","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Drug</th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Ascites</th>\n","      <th>Hepatomegaly</th>\n","      <th>Spiders</th>\n","      <th>Edema</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>999</td>\n","      <td>D-penicillamine</td>\n","      <td>21532</td>\n","      <td>M</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>2.3</td>\n","      <td>316.0</td>\n","      <td>3.35</td>\n","      <td>172.0</td>\n","      <td>1601.0</td>\n","      <td>179.80</td>\n","      <td>63.0</td>\n","      <td>394.0</td>\n","      <td>9.7</td>\n","      <td>3.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2574</td>\n","      <td>Placebo</td>\n","      <td>19237</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.9</td>\n","      <td>364.0</td>\n","      <td>3.54</td>\n","      <td>63.0</td>\n","      <td>1440.0</td>\n","      <td>134.85</td>\n","      <td>88.0</td>\n","      <td>361.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3428</td>\n","      <td>Placebo</td>\n","      <td>13727</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>3.3</td>\n","      <td>299.0</td>\n","      <td>3.55</td>\n","      <td>131.0</td>\n","      <td>1029.0</td>\n","      <td>119.35</td>\n","      <td>50.0</td>\n","      <td>199.0</td>\n","      <td>11.7</td>\n","      <td>4.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2576</td>\n","      <td>Placebo</td>\n","      <td>18460</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.6</td>\n","      <td>256.0</td>\n","      <td>3.50</td>\n","      <td>58.0</td>\n","      <td>1653.0</td>\n","      <td>71.30</td>\n","      <td>96.0</td>\n","      <td>269.0</td>\n","      <td>10.7</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>788</td>\n","      <td>Placebo</td>\n","      <td>16658</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>1.1</td>\n","      <td>346.0</td>\n","      <td>3.65</td>\n","      <td>63.0</td>\n","      <td>1181.0</td>\n","      <td>125.55</td>\n","      <td>96.0</td>\n","      <td>298.0</td>\n","      <td>10.6</td>\n","      <td>4.0</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n","0     999  D-penicillamine  21532   M       N            N       N     N   \n","1    2574          Placebo  19237   F       N            N       N     N   \n","2    3428          Placebo  13727   F       N            Y       Y     Y   \n","3    2576          Placebo  18460   F       N            N       N     N   \n","4     788          Placebo  16658   F       N            Y       N     N   \n","\n","   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n","0        2.3        316.0     3.35   172.0    1601.0  179.80           63.0   \n","1        0.9        364.0     3.54    63.0    1440.0  134.85           88.0   \n","2        3.3        299.0     3.55   131.0    1029.0  119.35           50.0   \n","3        0.6        256.0     3.50    58.0    1653.0   71.30           96.0   \n","4        1.1        346.0     3.65    63.0    1181.0  125.55           96.0   \n","\n","   Platelets  Prothrombin  Stage Status  \n","0      394.0          9.7    3.0      D  \n","1      361.0         11.0    3.0      C  \n","2      199.0         11.7    4.0      D  \n","3      269.0         10.7    3.0      C  \n","4      298.0         10.6    4.0      C  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Read in the initial competition training and test data as Pandas DataFrames \n","train_file_path = \"/kaggle/input/playground-series-s3e26/train.csv\"\n","train_df = pd.read_csv(train_file_path) \n","test_file_path = \"/kaggle/input/playground-series-s3e26/test.csv\"\n","test_df = pd.read_csv(test_file_path)\n","\n","# Remove the id column since it is not useful for prediction and might confuse the model in training\n","test_id_df = test_df['id'].astype(int)\n","train_df = train_df.drop('id', axis=1)\n","test_df = test_df.drop('id', axis=1)\n","\n","# Add data from the original dataset to our training set \n","orig_df = pd.read_csv(\"/kaggle/input/cirrhosis-patient-survival-prediction/cirrhosis.csv\")[train_df.columns]\n","\n","# Merge supplementary data\n","train_df = pd.concat(objs=[train_df, orig_df]).reset_index(drop=True)\n","\n","# Use the head method to visually see that the dataset has been loaded\n","train_df.head(5)"]},{"cell_type":"code","execution_count":2,"id":"7cb1f1d3","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.686813Z","iopub.status.busy":"2024-01-01T22:38:31.686406Z","iopub.status.idle":"2024-01-01T22:38:31.700289Z","shell.execute_reply":"2024-01-01T22:38:31.69928Z"},"papermill":{"duration":0.028645,"end_time":"2024-01-01T22:38:31.702881","exception":false,"start_time":"2024-01-01T22:38:31.674236","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["(8741, 19)\n"]}],"source":["# Combine the synthetic and original dataset to obtain more training data \n","train_df = pd.concat([train_df, orig_df]).reset_index(drop=True)\n","\n","print(train_df.shape)"]},{"cell_type":"code","execution_count":3,"id":"d965079a","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.727062Z","iopub.status.busy":"2024-01-01T22:38:31.726349Z","iopub.status.idle":"2024-01-01T22:38:31.760011Z","shell.execute_reply":"2024-01-01T22:38:31.75887Z"},"papermill":{"duration":0.04897,"end_time":"2024-01-01T22:38:31.762836","exception":false,"start_time":"2024-01-01T22:38:31.713866","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 8741 entries, 0 to 8740\n","Data columns (total 19 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   N_Days         8741 non-null   int64  \n"," 1   Drug           8529 non-null   object \n"," 2   Age            8741 non-null   int64  \n"," 3   Sex            8741 non-null   object \n"," 4   Ascites        8529 non-null   object \n"," 5   Hepatomegaly   8529 non-null   object \n"," 6   Spiders        8529 non-null   object \n"," 7   Edema          8741 non-null   object \n"," 8   Bilirubin      8741 non-null   float64\n"," 9   Cholesterol    8473 non-null   float64\n"," 10  Albumin        8741 non-null   float64\n"," 11  Copper         8525 non-null   float64\n"," 12  Alk_Phos       8529 non-null   float64\n"," 13  SGOT           8529 non-null   float64\n"," 14  Tryglicerides  8469 non-null   float64\n"," 15  Platelets      8719 non-null   float64\n"," 16  Prothrombin    8737 non-null   float64\n"," 17  Stage          8729 non-null   float64\n"," 18  Status         8741 non-null   object \n","dtypes: float64(10), int64(2), object(7)\n","memory usage: 1.3+ MB\n"]}],"source":["# Use info to see the number of categorical/text columns (any column with Dtype=object is text and will need to be converted to a number since ML models work only with numbers) \n","# Here you can also see if there are any missing values by looking at the number of non-null values in each column\n","train_df.info()\n","\n","# Make a list of categorical columns for future use (in the feature scaling section)\n","categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']"]},{"cell_type":"code","execution_count":4,"id":"e9299210","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.786733Z","iopub.status.busy":"2024-01-01T22:38:31.786337Z","iopub.status.idle":"2024-01-01T22:38:31.844246Z","shell.execute_reply":"2024-01-01T22:38:31.843039Z"},"papermill":{"duration":0.072734,"end_time":"2024-01-01T22:38:31.846741","exception":false,"start_time":"2024-01-01T22:38:31.774007","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>8741.000000</td>\n","      <td>8741.000000</td>\n","      <td>8741.000000</td>\n","      <td>8473.000000</td>\n","      <td>8741.000000</td>\n","      <td>8525.000000</td>\n","      <td>8529.000000</td>\n","      <td>8529.000000</td>\n","      <td>8469.000000</td>\n","      <td>8719.000000</td>\n","      <td>8737.000000</td>\n","      <td>8729.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2019.424093</td>\n","      <td>18388.468711</td>\n","      <td>2.654387</td>\n","      <td>351.832173</td>\n","      <td>3.543456</td>\n","      <td>84.902522</td>\n","      <td>1828.883621</td>\n","      <td>115.186369</td>\n","      <td>115.963632</td>\n","      <td>264.463012</td>\n","      <td>10.639201</td>\n","      <td>3.031733</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1095.607879</td>\n","      <td>3693.022293</td>\n","      <td>3.877583</td>\n","      <td>198.066724</td>\n","      <td>0.354724</td>\n","      <td>76.719279</td>\n","      <td>1922.259906</td>\n","      <td>49.448310</td>\n","      <td>53.505041</td>\n","      <td>88.555631</td>\n","      <td>0.808112</td>\n","      <td>0.867889</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>41.000000</td>\n","      <td>9598.000000</td>\n","      <td>0.300000</td>\n","      <td>120.000000</td>\n","      <td>1.960000</td>\n","      <td>4.000000</td>\n","      <td>289.000000</td>\n","      <td>26.350000</td>\n","      <td>33.000000</td>\n","      <td>62.000000</td>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1217.000000</td>\n","      <td>15574.000000</td>\n","      <td>0.700000</td>\n","      <td>248.000000</td>\n","      <td>3.350000</td>\n","      <td>39.000000</td>\n","      <td>834.000000</td>\n","      <td>75.950000</td>\n","      <td>84.000000</td>\n","      <td>209.000000</td>\n","      <td>10.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1831.000000</td>\n","      <td>18713.000000</td>\n","      <td>1.100000</td>\n","      <td>299.000000</td>\n","      <td>3.580000</td>\n","      <td>64.000000</td>\n","      <td>1181.000000</td>\n","      <td>108.500000</td>\n","      <td>104.000000</td>\n","      <td>263.000000</td>\n","      <td>10.600000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2689.000000</td>\n","      <td>20684.000000</td>\n","      <td>3.000000</td>\n","      <td>392.000000</td>\n","      <td>3.770000</td>\n","      <td>102.000000</td>\n","      <td>1860.000000</td>\n","      <td>137.950000</td>\n","      <td>139.000000</td>\n","      <td>316.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4795.000000</td>\n","      <td>28650.000000</td>\n","      <td>28.000000</td>\n","      <td>1775.000000</td>\n","      <td>4.640000</td>\n","      <td>588.000000</td>\n","      <td>13862.400000</td>\n","      <td>457.250000</td>\n","      <td>598.000000</td>\n","      <td>721.000000</td>\n","      <td>18.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days           Age    Bilirubin  Cholesterol      Albumin  \\\n","count  8741.000000   8741.000000  8741.000000  8473.000000  8741.000000   \n","mean   2019.424093  18388.468711     2.654387   351.832173     3.543456   \n","std    1095.607879   3693.022293     3.877583   198.066724     0.354724   \n","min      41.000000   9598.000000     0.300000   120.000000     1.960000   \n","25%    1217.000000  15574.000000     0.700000   248.000000     3.350000   \n","50%    1831.000000  18713.000000     1.100000   299.000000     3.580000   \n","75%    2689.000000  20684.000000     3.000000   392.000000     3.770000   \n","max    4795.000000  28650.000000    28.000000  1775.000000     4.640000   \n","\n","            Copper      Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  8525.000000   8529.000000  8529.000000    8469.000000  8719.000000   \n","mean     84.902522   1828.883621   115.186369     115.963632   264.463012   \n","std      76.719279   1922.259906    49.448310      53.505041    88.555631   \n","min       4.000000    289.000000    26.350000      33.000000    62.000000   \n","25%      39.000000    834.000000    75.950000      84.000000   209.000000   \n","50%      64.000000   1181.000000   108.500000     104.000000   263.000000   \n","75%     102.000000   1860.000000   137.950000     139.000000   316.000000   \n","max     588.000000  13862.400000   457.250000     598.000000   721.000000   \n","\n","       Prothrombin        Stage  \n","count  8737.000000  8729.000000  \n","mean     10.639201     3.031733  \n","std       0.808112     0.867889  \n","min       9.000000     1.000000  \n","25%      10.000000     2.000000  \n","50%      10.600000     3.000000  \n","75%      11.000000     4.000000  \n","max      18.000000     4.000000  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Use the describe method to generate statistical information such as standard deviation, mean, and min/max. \n","train_df.describe()"]},{"cell_type":"markdown","id":"4668edc5","metadata":{"papermill":{"duration":0.010904,"end_time":"2024-01-01T22:38:31.868884","exception":false,"start_time":"2024-01-01T22:38:31.85798","status":"completed"},"tags":[]},"source":["# Dataset Pre-Processing Task Overview\n","\n","1. Target Variable Label Encoding - Make sure the training labels are numerical, otherwise we cannot train the model. \n","2. Missing Feature Value Handling - Make sure the dataset is not missing any values, if so we cannot train the model.\n","3. Feature Scaling - Make sure the features are similar in numerical value, otherwise some ML models will struggle to weigh them appropriately during training.  \n","4. Categorical Attribute Handling (Should be done at the end since it may change the number of columns) - Make sure text values have been converted to numbers, otherwise ML algorithms cannot learn from text data.\n","5. Train/Test Split - Need to create a validation set for hyperparameter tuning and evaluation, otherwise the model will overfit the hyperparameters to the training dataset and your model will not generalize well to the real-world / Kaggle private test set. "]},{"cell_type":"markdown","id":"eb50f42f","metadata":{"papermill":{"duration":0.011025,"end_time":"2024-01-01T22:38:31.891239","exception":false,"start_time":"2024-01-01T22:38:31.880214","status":"completed"},"tags":[]},"source":["## Target Variable Label Encoding\n","\n","Problem: Since the Label column that we are trying to predict, 'Status', is a text column, we need to convert the column's values into integers so the machine learning model can process it (ML models do not support text input so we must convert the text into a numerical representation). We have three possible categories: Status_C, Status_CL, and Status_D. \n","\n","Possible Approaches:\n","1. Label Encoding - Convert each text category into an integer label. (Ex: 0, 1, 2)\n","2. Ordinal Encoding - Convert each text category into an integer label but with a particular order. Used when the categories have some quantitative order that can be taken advantage of like low, medium, high.   (Ex: low -> 0, medium-> 1, high -> 2)\n","3. One-Hot Encoding - Convert each text category into a separate column. For example, this is done in the softmax layer of a neural network. \n","\n","Chosen Approach: Label Encoding - There is no obvious ordering in the 'Status' column and ML libraries typically expect a single Label column which rules out one-hot encoding. Therefore we will use label encoding."]},{"cell_type":"code","execution_count":5,"id":"7dfd20a7","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.916889Z","iopub.status.busy":"2024-01-01T22:38:31.916497Z","iopub.status.idle":"2024-01-01T22:38:31.926681Z","shell.execute_reply":"2024-01-01T22:38:31.925442Z"},"papermill":{"duration":0.026803,"end_time":"2024-01-01T22:38:31.92924","exception":false,"start_time":"2024-01-01T22:38:31.902437","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","C     5429\n","D     2987\n","CL     325\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Check to see what the initial text categories are\n","train_df['Status'].value_counts()"]},{"cell_type":"code","execution_count":6,"id":"c80eef81","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:31.954754Z","iopub.status.busy":"2024-01-01T22:38:31.954326Z","iopub.status.idle":"2024-01-01T22:38:33.280621Z","shell.execute_reply":"2024-01-01T22:38:33.279334Z"},"papermill":{"duration":1.341829,"end_time":"2024-01-01T22:38:33.28323","exception":false,"start_time":"2024-01-01T22:38:31.941401","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","0    5429\n","2    2987\n","1     325\n","Name: count, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Use sklearn's LabelEncoder class to transform the Status column's values from strings to integers.\n","label_encoder = LabelEncoder() \n","train_df['Status'] = label_encoder.fit_transform(train_df['Status'])\n","\n","# Check that the label encoder transformation was applied correctly and the categories are now numbers\n","train_df['Status'].value_counts()"]},{"cell_type":"markdown","id":"d86631c1","metadata":{"papermill":{"duration":0.011504,"end_time":"2024-01-01T22:38:33.306454","exception":false,"start_time":"2024-01-01T22:38:33.29495","status":"completed"},"tags":[]},"source":["## Missing Feature Value Handling\n","\n","Problem: We need to check if there are any missing feature values since most machine learning algorithms cannot handle an empty cell with no value. For example, a Logistic Regression model would throw an error in training although some ML algorithms like XGBoost are implemented to train normally by substituting in a value as needed. \n","\n","Solution: We check for missing feature values by using the isna DataFrame method which returns a boolean DataFrame where each cell is True if the value is missing and False otherwise. We then apply the sum method to find the number of missing values in each column. \n","\n","In the synthetic dataset there are no missing values, however, after combining the original and synthetic datasets there are many missing values in various columns. We could try to replace missing values with median/mean values but for simplicity we choose to simply drop columns that are missing values. "]},{"cell_type":"code","execution_count":7,"id":"5b0f78fd","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.331928Z","iopub.status.busy":"2024-01-01T22:38:33.331281Z","iopub.status.idle":"2024-01-01T22:38:33.346775Z","shell.execute_reply":"2024-01-01T22:38:33.345115Z"},"papermill":{"duration":0.0312,"end_time":"2024-01-01T22:38:33.349392","exception":false,"start_time":"2024-01-01T22:38:33.318192","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column: \n","N_Days             0\n","Drug             212\n","Age                0\n","Sex                0\n","Ascites          212\n","Hepatomegaly     212\n","Spiders          212\n","Edema              0\n","Bilirubin          0\n","Cholesterol      268\n","Albumin            0\n","Copper           216\n","Alk_Phos         212\n","SGOT             212\n","Tryglicerides    272\n","Platelets         22\n","Prothrombin        4\n","Stage             12\n","Status             0\n","dtype: int64\n"]}],"source":["# Check for missing feature values\n","print(\"Number of missing feature values by column: \")\n","print(train_df.isna().sum())"]},{"cell_type":"code","execution_count":8,"id":"d926afb0","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.376618Z","iopub.status.busy":"2024-01-01T22:38:33.376139Z","iopub.status.idle":"2024-01-01T22:38:33.388845Z","shell.execute_reply":"2024-01-01T22:38:33.387951Z"},"papermill":{"duration":0.02952,"end_time":"2024-01-01T22:38:33.391503","exception":false,"start_time":"2024-01-01T22:38:33.361983","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column in test data: 2066\n"]}],"source":["# Alternatively we can check the total number of missing feature values in the dataset this way. \n","print(\"Number of missing feature values by column in test data:\", train_df.isna().sum().sum())"]},{"cell_type":"code","execution_count":9,"id":"c61ba7cf","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.417582Z","iopub.status.busy":"2024-01-01T22:38:33.416793Z","iopub.status.idle":"2024-01-01T22:38:33.446946Z","shell.execute_reply":"2024-01-01T22:38:33.445723Z"},"papermill":{"duration":0.046204,"end_time":"2024-01-01T22:38:33.449607","exception":false,"start_time":"2024-01-01T22:38:33.403403","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["N_Days           0\n","Drug             0\n","Age              0\n","Sex              0\n","Ascites          0\n","Hepatomegaly     0\n","Spiders          0\n","Edema            0\n","Bilirubin        0\n","Cholesterol      0\n","Albumin          0\n","Copper           0\n","Alk_Phos         0\n","SGOT             0\n","Tryglicerides    0\n","Platelets        0\n","Prothrombin      0\n","Stage            0\n","Status           0\n","dtype: int64\n","(8457, 19)\n"]}],"source":["# Remove rows that are missing any feature values, then shuffle the training examples\n","\n","train_df = train_df.dropna()\n","train_df = train_df.sample(frac = 1).reset_index(drop = True)\n","\n","print(train_df.isna().sum())\n","print(train_df.shape)"]},{"cell_type":"markdown","id":"bfef1e3f","metadata":{"papermill":{"duration":0.011928,"end_time":"2024-01-01T22:38:33.473956","exception":false,"start_time":"2024-01-01T22:38:33.462028","status":"completed"},"tags":[]},"source":["## Feature Scaling\n","\n","Problem: Typically since feature column values are combined (often by adding them together) to create the final classification, the ML model will perform better if the features are on the same scale. (Ex: The ML model would struggle to scale values correctly if one column's values was in the billions and another column had values from 1-10)\n","\n","Possible Feature Scaling Approaches: \n","1. Min-Max Scaling (Default)- Scales the data to a fixed range between two values (typically 0 and 1). Most useful for neural networks.\n","2. Standardization (Default) - Scales the data so that the mean is 0 and the standard deviation is 1. Most useful for algorithms that assume a normal distribution of data, such as SVMs and logistic regression.\n","3. Robust Scaling (Advanced) - Scaling based on median and IQR. Most useful for handling significant outliers.\n","4. MaxAbsScaler (Advanced) - Scales each feature based on its maximum absolute value. Useful for sparse data. \n","\n","Chosen Approach: Standardization - Since XGBoost's decision tree classification uses splitting which occurs within a column, different column values do not interact with each other and therefore scaling the features is not necessary. However, since we are using Logistic Regression as our baseline model and features do interact in training we will need to do feature scaling. Since we are doing feature scaling specifically for our logistic regression model we will use the Standardization approach which is a commonly used approach that is effective for many datasets. "]},{"cell_type":"code","execution_count":10,"id":"75827c60","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.500505Z","iopub.status.busy":"2024-01-01T22:38:33.50008Z","iopub.status.idle":"2024-01-01T22:38:33.569988Z","shell.execute_reply":"2024-01-01T22:38:33.56896Z"},"papermill":{"duration":0.086428,"end_time":"2024-01-01T22:38:33.572677","exception":false,"start_time":"2024-01-01T22:38:33.486249","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.010821</td>\n","      <td>0.037017</td>\n","      <td>-0.010934</td>\n","      <td>0.002893</td>\n","      <td>-0.023394</td>\n","      <td>-0.003929</td>\n","      <td>-0.008993</td>\n","      <td>-0.033165</td>\n","      <td>-0.012770</td>\n","      <td>-0.011153</td>\n","      <td>-0.004406</td>\n","      <td>0.004626</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.992278</td>\n","      <td>0.971129</td>\n","      <td>0.994747</td>\n","      <td>1.011233</td>\n","      <td>1.013070</td>\n","      <td>1.008822</td>\n","      <td>1.008831</td>\n","      <td>0.991336</td>\n","      <td>0.983303</td>\n","      <td>0.997158</td>\n","      <td>0.992475</td>\n","      <td>0.989000</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-1.813057</td>\n","      <td>-2.374549</td>\n","      <td>-0.604929</td>\n","      <td>-1.170022</td>\n","      <td>-4.528214</td>\n","      <td>-1.053857</td>\n","      <td>-0.802480</td>\n","      <td>-1.799366</td>\n","      <td>-1.550943</td>\n","      <td>-2.311224</td>\n","      <td>-2.048754</td>\n","      <td>-2.348783</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-0.723859</td>\n","      <td>-0.712962</td>\n","      <td>-0.501643</td>\n","      <td>-0.524250</td>\n","      <td>-0.560268</td>\n","      <td>-0.598507</td>\n","      <td>-0.524124</td>\n","      <td>-0.814404</td>\n","      <td>-0.597601</td>\n","      <td>-0.637606</td>\n","      <td>-0.796753</td>\n","      <td>-1.193449</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>-0.132241</td>\n","      <td>0.120541</td>\n","      <td>-0.398357</td>\n","      <td>-0.266951</td>\n","      <td>0.067752</td>\n","      <td>-0.260248</td>\n","      <td>-0.357841</td>\n","      <td>-0.167548</td>\n","      <td>-0.223741</td>\n","      <td>-0.068349</td>\n","      <td>-0.045553</td>\n","      <td>-0.038115</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.607281</td>\n","      <td>0.629423</td>\n","      <td>0.092252</td>\n","      <td>0.192153</td>\n","      <td>0.638680</td>\n","      <td>0.221122</td>\n","      <td>0.005220</td>\n","      <td>0.460075</td>\n","      <td>0.411820</td>\n","      <td>0.591990</td>\n","      <td>0.455247</td>\n","      <td>1.117219</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2.527300</td>\n","      <td>2.787969</td>\n","      <td>6.547631</td>\n","      <td>7.179603</td>\n","      <td>3.122215</td>\n","      <td>6.543979</td>\n","      <td>6.272858</td>\n","      <td>6.924587</td>\n","      <td>9.010593</td>\n","      <td>3.392738</td>\n","      <td>5.713651</td>\n","      <td>1.117219</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days          Age    Bilirubin  Cholesterol      Albumin  \\\n","count  5271.000000  5271.000000  5271.000000  5271.000000  5271.000000   \n","mean      0.010821     0.037017    -0.010934     0.002893    -0.023394   \n","std       0.992278     0.971129     0.994747     1.011233     1.013070   \n","min      -1.813057    -2.374549    -0.604929    -1.170022    -4.528214   \n","25%      -0.723859    -0.712962    -0.501643    -0.524250    -0.560268   \n","50%      -0.132241     0.120541    -0.398357    -0.266951     0.067752   \n","75%       0.607281     0.629423     0.092252     0.192153     0.638680   \n","max       2.527300     2.787969     6.547631     7.179603     3.122215   \n","\n","            Copper     Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  5271.000000  5271.000000  5271.000000    5271.000000  5271.000000   \n","mean     -0.003929    -0.008993    -0.033165      -0.012770    -0.011153   \n","std       1.008822     1.008831     0.991336       0.983303     0.997158   \n","min      -1.053857    -0.802480    -1.799366      -1.550943    -2.311224   \n","25%      -0.598507    -0.524124    -0.814404      -0.597601    -0.637606   \n","50%      -0.260248    -0.357841    -0.167548      -0.223741    -0.068349   \n","75%       0.221122     0.005220     0.460075       0.411820     0.591990   \n","max       6.543979     6.272858     6.924587       9.010593     3.392738   \n","\n","       Prothrombin        Stage  \n","count  5271.000000  5271.000000  \n","mean     -0.004406     0.004626  \n","std       0.992475     0.989000  \n","min      -2.048754    -2.348783  \n","25%      -0.796753    -1.193449  \n","50%      -0.045553    -0.038115  \n","75%       0.455247     1.117219  \n","max       5.713651     1.117219  "]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler \n","\n","# Here we find the numerical columns that need to be scaled by removing the text columns from the list of total columns in the df.\n","numerical_cols = train_df.columns.difference(categorical_cols)\n","\n","# Now we use the StandardScaler class from sklearn to transform our numerical columns to the a Standardized scale\n","std_scaler = StandardScaler() \n","std_scaler.fit(train_df[numerical_cols])\n","\n","train_df[numerical_cols] = std_scaler.transform(train_df[numerical_cols])\n","test_df[numerical_cols] = std_scaler.transform(test_df[numerical_cols])\n","                      \n","# Confirm the transformation was successful by seeing if the mean = 0 and std = 1 for numerical columns\n","test_df.describe()"]},{"cell_type":"markdown","id":"9f50f846","metadata":{"papermill":{"duration":0.012382,"end_time":"2024-01-01T22:38:33.59762","exception":false,"start_time":"2024-01-01T22:38:33.585238","status":"completed"},"tags":[]},"source":["## Handling Categorical Attributes/Columns \n","Problem: ML models cannot handle text data naturally, they can only handle numbers so we need to convert text data into some numerical representation that still contains the relevant information. \n","\n","Possible Approaches: The main approaches for categorical attribute handling are \n","1. Ordinal Encoding - Useful when the categories correspond to an ascending or descending order. \n","2. One-Hot Encoding (Default Choice) - For each categorical column, convert it into multiple columns, one for each possible category. This is used when the categories do not have an obvious logical order. \n","3. Numerical Feature Replacement (Advanced) - In cases where the number of categories is cery large (hundreds or thousands) one should consider replacing the categorical columns with a numerical column that converts each category into some number. For example, one could convert a country code into the country's population. \n","4. Embedding Replacement (Advanced) - Alternatively, one can replace categories with embeddings, which are low dimensional vectors that represent the category. \n","\n","Chosen Approach : One-Hot Encoding - In this case, we use a one-hot encoding since none of the categories seem to have an order to the classes (ruling out ordinal encoding) and the number of categories for each column is low (under 10 for all categorical columns) which rules out needing advanced methods. "]},{"cell_type":"code","execution_count":11,"id":"8fd7cf70","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.624734Z","iopub.status.busy":"2024-01-01T22:38:33.62369Z","iopub.status.idle":"2024-01-01T22:38:33.640554Z","shell.execute_reply":"2024-01-01T22:38:33.639207Z"},"papermill":{"duration":0.033368,"end_time":"2024-01-01T22:38:33.643364","exception":false,"start_time":"2024-01-01T22:38:33.609996","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["N_Days           461\n","Drug               2\n","Age              391\n","Sex                2\n","Ascites            2\n","Hepatomegaly       2\n","Spiders            2\n","Edema              3\n","Bilirubin        111\n","Cholesterol      226\n","Albumin          160\n","Copper           171\n","Alk_Phos         364\n","SGOT             206\n","Tryglicerides    154\n","Platelets        227\n","Prothrombin       49\n","Stage              4\n","Status             3\n","dtype: int64\n"]}],"source":["# Confirm that the number of categories in the categorical columns is manageable (< 100) since we will be adding a column to the df for each category\n","unique_values_per_column = train_df.nunique()\n","print(unique_values_per_column)"]},{"cell_type":"code","execution_count":12,"id":"4e8fc4c6","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.671355Z","iopub.status.busy":"2024-01-01T22:38:33.670035Z","iopub.status.idle":"2024-01-01T22:38:33.711705Z","shell.execute_reply":"2024-01-01T22:38:33.710405Z"},"papermill":{"duration":0.058101,"end_time":"2024-01-01T22:38:33.714311","exception":false,"start_time":"2024-01-01T22:38:33.65621","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5271 entries, 0 to 5270\n","Data columns (total 25 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   N_Days                5271 non-null   float64\n"," 1   Age                   5271 non-null   float64\n"," 2   Bilirubin             5271 non-null   float64\n"," 3   Cholesterol           5271 non-null   float64\n"," 4   Albumin               5271 non-null   float64\n"," 5   Copper                5271 non-null   float64\n"," 6   Alk_Phos              5271 non-null   float64\n"," 7   SGOT                  5271 non-null   float64\n"," 8   Tryglicerides         5271 non-null   float64\n"," 9   Platelets             5271 non-null   float64\n"," 10  Prothrombin           5271 non-null   float64\n"," 11  Stage                 5271 non-null   float64\n"," 12  Drug_D-penicillamine  5271 non-null   bool   \n"," 13  Drug_Placebo          5271 non-null   bool   \n"," 14  Sex_F                 5271 non-null   bool   \n"," 15  Sex_M                 5271 non-null   bool   \n"," 16  Ascites_N             5271 non-null   bool   \n"," 17  Ascites_Y             5271 non-null   bool   \n"," 18  Hepatomegaly_N        5271 non-null   bool   \n"," 19  Hepatomegaly_Y        5271 non-null   bool   \n"," 20  Spiders_N             5271 non-null   bool   \n"," 21  Spiders_Y             5271 non-null   bool   \n"," 22  Edema_N               5271 non-null   bool   \n"," 23  Edema_S               5271 non-null   bool   \n"," 24  Edema_Y               5271 non-null   bool   \n","dtypes: bool(13), float64(12)\n","memory usage: 561.2 KB\n"]}],"source":["# Convert the categorical columns into one-hot encodings using the get_dummies function\n","status = train_df['Status']\n","train_df_dummies = pd.get_dummies(train_df.drop('Status', axis=1))\n","train_df = pd.concat([train_df_dummies, status], axis=1)\n","test_df = pd.get_dummies(test_df)\n","\n","# Confirm the transformation was successful\n","test_df.info()"]},{"cell_type":"markdown","id":"5949514c","metadata":{"papermill":{"duration":0.012403,"end_time":"2024-01-01T22:38:33.739228","exception":false,"start_time":"2024-01-01T22:38:33.726825","status":"completed"},"tags":[]},"source":["## Train/Test Split\n","\n","Problem: We need to split the dataset into a training dataset and a testing/tuning dataset. If we were not to do this and did both training / tuning / evaluation with the same dataset, the final model would likely be overfitted and would not generalize well to the real-world / the Kaggle competition private test set. \n","\n","Possible Approaches\n","1. K-Fold Cross-Validation - In this approach the dataset is divided into k equal-sized subsets. Then we train the model k times, each time using a different subset as the validation set and the other k-1 subsets as the training set. Finally we use the average score among all k models as the final score. This approach is commonly used since it essentially utilizes  more of the training data for validation (90% vs. 80%). Typical values range are k=5 or k=10. \n","2. Train/Validation/Test Split - In this approach we split the dataset into three distinct sets: a training set for training the model, a validation set for tuning the hyperparameters and a test set for evaluating the final model. By separating the test set and validation set we reduce / avoid overfitting to the test set. A typical split varies but could be 80/10/10 or 70/15/15.\n","\n","Chosen Approach : Initially we used the train/validation/test approach since it is simpler to implement for manual hyperparameter tuning. However later we noticed that the log-loss scores fluctuated a lot depending on what training examples were placed in the validation set, so to reduce this effect we switched to K-Fold Cross Validation for hyperparameter tuning with k=5. "]},{"cell_type":"code","execution_count":13,"id":"007c023c","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.76707Z","iopub.status.busy":"2024-01-01T22:38:33.766026Z","iopub.status.idle":"2024-01-01T22:38:33.88583Z","shell.execute_reply":"2024-01-01T22:38:33.884514Z"},"papermill":{"duration":0.136896,"end_time":"2024-01-01T22:38:33.88869","exception":false,"start_time":"2024-01-01T22:38:33.751794","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Set parameters for k-folds cross-validation\n","kfold = StratifiedKFold(n_splits=10, shuffle=True)\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split the training set into a training and validation set \n","X = train_df.drop(\"Status\", axis=1)\n","y = train_df[\"Status\"]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y,  test_size = 0.2, random_state=8)\n"]},{"cell_type":"markdown","id":"5c051d11","metadata":{"papermill":{"duration":0.012232,"end_time":"2024-01-01T22:38:33.913695","exception":false,"start_time":"2024-01-01T22:38:33.901463","status":"completed"},"tags":[]},"source":["# Logistic Regression Baseline Model\n","Default Model Performance: 0.526\n","\n","Tuned Model Performance: 0.526\n","\n","Hyperparameter Tuning: \n","Here we manually tuned the most important parameters for Logistic Regression (C, max_iter, class_weight, tol, penalty, solver) but were unable to significantly increase performance (still around 0.506)\n","\n","Conclusion: We set the baseline of model performance at 0.506 log-loss since Logistic Regression is the simplest model we used on the dataset. Since the performance was so weak, we decided to not include Logistic Regression in the final ensemble of models for competition submission.  "]},{"cell_type":"code","execution_count":14,"id":"cbe3d0ca","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:33.942851Z","iopub.status.busy":"2024-01-01T22:38:33.942417Z","iopub.status.idle":"2024-01-01T22:38:36.318006Z","shell.execute_reply":"2024-01-01T22:38:36.31648Z"},"papermill":{"duration":2.39394,"end_time":"2024-01-01T22:38:36.322299","exception":false,"start_time":"2024-01-01T22:38:33.928359","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Log loss scores for each fold: [0.56819077 0.52996612 0.52306784 0.50775237 0.52983372]\n","Average log loss: 0.531762164857363\n"]}],"source":["from sklearn.linear_model import LogisticRegression \n","from sklearn.metrics import log_loss\n","from sklearn.model_selection import cross_val_score\n","\n","# Train the model using sci-kit learn's Logistic Regression model \n","model = LogisticRegression(C=1, max_iter = 1000)\n","\n","# Use neg_log_loss as the scoring parameter\n","scores = cross_val_score(model, X, y, cv=5, scoring='neg_log_loss')\n","\n","print(\"Log loss scores for each fold:\", scores*-1)\n","print(\"Average log loss:\", scores.mean()*-1)\n"]},{"cell_type":"markdown","id":"af83e587","metadata":{"papermill":{"duration":0.0283,"end_time":"2024-01-01T22:38:36.379689","exception":false,"start_time":"2024-01-01T22:38:36.351389","status":"completed"},"tags":[]},"source":["# Random Forest Model \n","Default Model Performance: 0.550\n","\n","Tuned Model Performance: 0.482\n","\n","Hyperparameter Tuning:\n","We manually tuned hyperparameters starting from the most important hyperparameters (n_estimators) and working through the rest (max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf, min_weight_fraction_leaf). We only found significant improvements in performance by tuning the n_estimator hyperparameter which improved performance from 0.477 -> 0.473.\n","\n","Conclusion: The overall conclusion is that the Random Forest model performed significantly better than the baseline Logistic Regression model with a log-loss of 0.434 compared to 0.506. However, this model still performed significantly worse than the best XGBoost and LGBM models so we decided to exclude the Random Forest from the final ensemble for competition submission. \n"]},{"cell_type":"code","execution_count":15,"id":"9f8801af","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:36.430539Z","iopub.status.busy":"2024-01-01T22:38:36.430098Z","iopub.status.idle":"2024-01-01T22:38:44.764358Z","shell.execute_reply":"2024-01-01T22:38:44.763105Z"},"papermill":{"duration":8.359324,"end_time":"2024-01-01T22:38:44.767076","exception":false,"start_time":"2024-01-01T22:38:36.407752","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Log loss scores for each fold: [0.55774756 0.64153069 0.508682   0.42547163 0.51891893]\n","Average log loss: 0.5304701608990016\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Train the model using sci-kit learn's Random Forest model \n","rf_classifier = RandomForestClassifier()\n","\n","# Use neg_log_loss as the scoring parameter\n","scores = cross_val_score(rf_classifier, X, y, cv=5, scoring='neg_log_loss')\n","\n","print(\"Log loss scores for each fold:\", scores*-1)\n","print(\"Average log loss:\", scores.mean()*-1)"]},{"cell_type":"markdown","id":"d49aa2f7","metadata":{"papermill":{"duration":0.012683,"end_time":"2024-01-01T22:38:44.79306","exception":false,"start_time":"2024-01-01T22:38:44.780377","status":"completed"},"tags":[]},"source":["# XGBoost Model\n","Default Model Performance: 0.515\n","\n","Tuned Model Performance: 0.431\n","\n","Approach: Initially we tuned the model hyperparameters manually as before which is an approach I've seen suggested in some ML books such as Corey Wade's XGBoost book. However, we were not able to tune the model beyond ~0.42 score. \n","\n","Conclusion: After comparing to other XGboost notebooks it was clear in order to tune the model hyperparameters manually one needs a great deal of experience. For example, in the method I have been using where you tune hyperparameters one at a time by simply changing the numbers in the XGBClassifier(n_estimators=700) constructor call, n_estimators=700 performs very poorly and therefore is discarded early in the process. However, it turns out n_estimators=700 performs very well *if* the learning_rate is also tuned at the same time. This suggests I need to improve my understanding of how hyperparameters interact and automate some of my process using GridSearchCV for future hyperparameter tuning. \n"]},{"cell_type":"code","execution_count":16,"id":"12656d00","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:44.821901Z","iopub.status.busy":"2024-01-01T22:38:44.821488Z","iopub.status.idle":"2024-01-01T22:38:56.014994Z","shell.execute_reply":"2024-01-01T22:38:56.014074Z"},"papermill":{"duration":11.211023,"end_time":"2024-01-01T22:38:56.017868","exception":false,"start_time":"2024-01-01T22:38:44.806845","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Log loss scores for each fold: [0.4428934  0.41668534 0.39935254 0.39499659 0.4007548 ]\n","Average log loss: 0.4109365342845749\n"]}],"source":["from xgboost import XGBClassifier \n","\n","# Train the model using xgboost's XGBClassifier model \n","xgb_model = XGBClassifier(n_estimators=540, max_depth=6, learning_rate=0.04, colsample_bytree=0.15, min_child_weight=17, subsample=0.7) \n","\n","# Use neg_log_loss as the scoring parameter\n","scores = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_log_loss')\n","\n","print(\"Log loss scores for each fold:\", scores*-1)\n","print(\"Average log loss:\", scores.mean()*-1)"]},{"cell_type":"markdown","id":"0e741c82","metadata":{"papermill":{"duration":0.013234,"end_time":"2024-01-01T22:38:56.044671","exception":false,"start_time":"2024-01-01T22:38:56.031437","status":"completed"},"tags":[]},"source":["# LGBM Model\n","Default Model Performance: 0.472\n","\n","Tuned Model Performance: 0.425\n","\n","Approach  / Conclusion: The approach and conclusions are similar to those found in the XGBoost section. "]},{"cell_type":"code","execution_count":17,"id":"b6b4236f","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:38:56.073816Z","iopub.status.busy":"2024-01-01T22:38:56.071884Z","iopub.status.idle":"2024-01-01T22:39:31.752759Z","shell.execute_reply":"2024-01-01T22:39:31.751396Z"},"papermill":{"duration":35.711157,"end_time":"2024-01-01T22:39:31.768701","exception":false,"start_time":"2024-01-01T22:38:56.057544","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Log loss scores for each fold: [0.43047962 0.40882054 0.3932935  0.38752345 0.40013281]\n","Average log loss: 0.4040499824421994\n"]}],"source":["import lightgbm as lgb\n","\n","# Define your parameters\n","lgb_params = {\n","    'max_depth': 15,\n","    'min_child_samples': 13,\n","    'learning_rate': 0.05285597081335651,\n","    'n_estimators': 294,\n","    'min_child_weight': 5,\n","    'colsample_bytree': 0.10012816493265511,\n","    'reg_alpha': 0.8767668608061822,\n","    'reg_lambda': 0.8705834466355764\n","}\n","\n","# Create the LGBMClassifier with the specified parameters\n","lgbm_model = lgb.LGBMClassifier(**lgb_params)\n","\n","# Use neg_log_loss as the scoring parameter\n","scores = cross_val_score(lgbm_model, X, y, cv=5, scoring='neg_log_loss')\n","\n","print(\"Log loss scores for each fold:\", scores*-1)\n","print(\"Average log loss:\", scores.mean()*-1)"]},{"cell_type":"markdown","id":"dd132976","metadata":{"papermill":{"duration":0.012754,"end_time":"2024-01-01T22:39:31.794686","exception":false,"start_time":"2024-01-01T22:39:31.781932","status":"completed"},"tags":[]},"source":["# Ensemble Model\n","\n","Approach: Since our current Logistic Regression and Random Forest models are significantly weaker than the XGBoost and LGBM models, we will only use the tuned XGBoost / LGBM models in the Ensemble. Further work will involve trying to tune the other models further so they can be included in the Ensemble. \n","\n","Tuned Ensemble Model Score: 0.431"]},{"cell_type":"code","execution_count":18,"id":"d271e778","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:39:31.889356Z","iopub.status.busy":"2024-01-01T22:39:31.888905Z","iopub.status.idle":"2024-01-01T22:40:25.940852Z","shell.execute_reply":"2024-01-01T22:40:25.939682Z"},"papermill":{"duration":54.082163,"end_time":"2024-01-01T22:40:25.95616","exception":false,"start_time":"2024-01-01T22:39:31.873997","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Log loss scores for each fold: [0.43433158 0.41019475 0.39442166 0.38922968 0.39799309]\n","Average log loss: 0.4052341498164491\n"]}],"source":["from sklearn.ensemble import VotingClassifier\n","\n","# Train an Ensemble model using a combination of the XGBoost and LGBM Classifiers, voting='soft' is used since we are predicting probabilities, not the actual classes\n","ensemble_model = VotingClassifier(\n","    estimators=[\n","        ('lgb', lgbm_model),\n","        ('xgb', xgb_model),\n","    ],\n","    voting='soft'\n",")\n","\n","# Use neg_log_loss as the scoring parameter\n","scores = cross_val_score(ensemble_model, X, y, cv=5, scoring='neg_log_loss')\n","\n","print(\"Log loss scores for each fold:\", scores*-1)\n","print(\"Average log loss:\", scores.mean()*-1)"]},{"cell_type":"code","execution_count":19,"id":"93ec3fc1","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:40:25.985357Z","iopub.status.busy":"2024-01-01T22:40:25.984913Z","iopub.status.idle":"2024-01-01T22:40:37.749235Z","shell.execute_reply":"2024-01-01T22:40:37.747862Z"},"papermill":{"duration":11.782726,"end_time":"2024-01-01T22:40:37.752127","exception":false,"start_time":"2024-01-01T22:40:25.969401","status":"completed"},"tags":[]},"outputs":[],"source":["# Train the final Ensemble model for competition submission using all the data, including data from the validation set \n","ensemble_model_final = VotingClassifier(\n","    estimators=[\n","        ('lgb', lgbm_model),\n","        ('xgb', xgb_model),\n","    ],\n","    voting='soft'\n",")\n","\n","ensemble_model_final.fit(X, y)\n","\n","# Make predictions using the ensemble\n","y_pred_ensemble_final = ensemble_model_final.predict_proba(test_df)"]},{"cell_type":"markdown","id":"e199531a","metadata":{"papermill":{"duration":0.013545,"end_time":"2024-01-01T22:40:37.779129","exception":false,"start_time":"2024-01-01T22:40:37.765584","status":"completed"},"tags":[]},"source":["# Kaggle Submission Processing\n","1. Create a submission dataframe from the model's predictions \n","2. Concatenate the data id column values to adhere to submission formatting requirements\n","3. Convert the submission dataframe into a csv file for submission \n","4. Now in order to submit to Kaggle, save the notebook and navigate to the Submissions page for this competition and click 'Submit Prediction' in the top-right corner -> Notebook -> Submit. "]},{"cell_type":"code","execution_count":20,"id":"171d8f88","metadata":{"execution":{"iopub.execute_input":"2024-01-01T22:40:37.807252Z","iopub.status.busy":"2024-01-01T22:40:37.806791Z","iopub.status.idle":"2024-01-01T22:40:37.86404Z","shell.execute_reply":"2024-01-01T22:40:37.862759Z"},"papermill":{"duration":0.074622,"end_time":"2024-01-01T22:40:37.867025","exception":false,"start_time":"2024-01-01T22:40:37.792403","status":"completed"},"tags":[]},"outputs":[],"source":["# Modify the probability predictions into the submission format \n","submission_df = pd.DataFrame(y_pred_ensemble_final, columns=['Status_C', 'Status_CL', 'Status_D'])\n","final_submission_df = pd.concat([test_id_df, submission_df], axis=1)\n","final_submission_df.head(10)\n","\n","# Create a submission.csv file that Kaggle will automatically evaluate for submission\n","final_submission_df.to_csv('submission.csv', index = False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7000181,"sourceId":60893,"sourceType":"competition"},{"datasetId":3873965,"sourceId":6724823,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":131.270682,"end_time":"2024-01-01T22:40:38.703458","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-01-01T22:38:27.432776","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}