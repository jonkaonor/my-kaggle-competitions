{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jonathankao/dec-2023-tabular-xgboost-beginner-friendly?scriptVersionId=156129283\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"0a879567","metadata":{"papermill":{"duration":0.008907,"end_time":"2023-12-23T04:24:24.537407","exception":false,"start_time":"2023-12-23T04:24:24.5285","status":"completed"},"tags":[]},"source":["# Task Overview \n","Task: In this synthetic dataset based off of a real dataset funded by the Mayo Clinic, each example represents both general and survival information about a patient that has liver cirrhosis, a condition involving prolonged liver damage. The goal is to train a machine learning model that can predict the patient's current survival status based on the data features. \n","\n","Approach Overview - This is a beginner-friendly notebook that provides a good starting template to work from as it only includes the essential components and has basic explanations for each step. The notebook has data loading, dataset pre-processing, and trains three models: Logistic Regression, Random Forest, and XGBoost. The model predictions are then prepared for final competition submission. Currently in the process of adding hyperparameter tuning for each model and combining the models together to form an ensemble.  \n","\n","Results Log\n","1. Initial Logistic Regression Model - ~.52\n","2. Initial Random Forest Model - ~.50\n","3. Initial XGBoost Model - ~.42"]},{"cell_type":"markdown","id":"152a785f","metadata":{"papermill":{"duration":0.008874,"end_time":"2023-12-23T04:24:24.555003","exception":false,"start_time":"2023-12-23T04:24:24.546129","status":"completed"},"tags":[]},"source":["# Dataset Loading"]},{"cell_type":"code","execution_count":1,"id":"e0c04ae8","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:24.574344Z","iopub.status.busy":"2023-12-23T04:24:24.573603Z","iopub.status.idle":"2023-12-23T04:24:25.124995Z","shell.execute_reply":"2023-12-23T04:24:25.123831Z"},"papermill":{"duration":0.564027,"end_time":"2023-12-23T04:24:25.127683","exception":false,"start_time":"2023-12-23T04:24:24.563656","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Drug</th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Ascites</th>\n","      <th>Hepatomegaly</th>\n","      <th>Spiders</th>\n","      <th>Edema</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>999</td>\n","      <td>D-penicillamine</td>\n","      <td>21532</td>\n","      <td>M</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>2.3</td>\n","      <td>316.0</td>\n","      <td>3.35</td>\n","      <td>172.0</td>\n","      <td>1601.0</td>\n","      <td>179.80</td>\n","      <td>63.0</td>\n","      <td>394.0</td>\n","      <td>9.7</td>\n","      <td>3.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2574</td>\n","      <td>Placebo</td>\n","      <td>19237</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.9</td>\n","      <td>364.0</td>\n","      <td>3.54</td>\n","      <td>63.0</td>\n","      <td>1440.0</td>\n","      <td>134.85</td>\n","      <td>88.0</td>\n","      <td>361.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3428</td>\n","      <td>Placebo</td>\n","      <td>13727</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>3.3</td>\n","      <td>299.0</td>\n","      <td>3.55</td>\n","      <td>131.0</td>\n","      <td>1029.0</td>\n","      <td>119.35</td>\n","      <td>50.0</td>\n","      <td>199.0</td>\n","      <td>11.7</td>\n","      <td>4.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2576</td>\n","      <td>Placebo</td>\n","      <td>18460</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.6</td>\n","      <td>256.0</td>\n","      <td>3.50</td>\n","      <td>58.0</td>\n","      <td>1653.0</td>\n","      <td>71.30</td>\n","      <td>96.0</td>\n","      <td>269.0</td>\n","      <td>10.7</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>788</td>\n","      <td>Placebo</td>\n","      <td>16658</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>1.1</td>\n","      <td>346.0</td>\n","      <td>3.65</td>\n","      <td>63.0</td>\n","      <td>1181.0</td>\n","      <td>125.55</td>\n","      <td>96.0</td>\n","      <td>298.0</td>\n","      <td>10.6</td>\n","      <td>4.0</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n","0     999  D-penicillamine  21532   M       N            N       N     N   \n","1    2574          Placebo  19237   F       N            N       N     N   \n","2    3428          Placebo  13727   F       N            Y       Y     Y   \n","3    2576          Placebo  18460   F       N            N       N     N   \n","4     788          Placebo  16658   F       N            Y       N     N   \n","\n","   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n","0        2.3        316.0     3.35   172.0    1601.0  179.80           63.0   \n","1        0.9        364.0     3.54    63.0    1440.0  134.85           88.0   \n","2        3.3        299.0     3.55   131.0    1029.0  119.35           50.0   \n","3        0.6        256.0     3.50    58.0    1653.0   71.30           96.0   \n","4        1.1        346.0     3.65    63.0    1181.0  125.55           96.0   \n","\n","   Platelets  Prothrombin  Stage Status  \n","0      394.0          9.7    3.0      D  \n","1      361.0         11.0    3.0      C  \n","2      199.0         11.7    4.0      D  \n","3      269.0         10.7    3.0      C  \n","4      298.0         10.6    4.0      C  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Read in the training and test data as Pandas DataFrames \n","train_file_path = \"/kaggle/input/playground-series-s3e26/train.csv\"\n","train_df = pd.read_csv(train_file_path) \n","test_file_path = \"/kaggle/input/playground-series-s3e26/test.csv\"\n","test_df = pd.read_csv(test_file_path)\n","\n","# Remove the id column since it is not useful for prediction\n","test_id_df = test_df['id'].astype(int)\n","train_df = train_df.drop('id', axis=1)\n","test_df = test_df.drop('id', axis=1)\n","\n","# Use the head method to visually see that the dataset has been loaded\n","train_df.head(5)"]},{"cell_type":"code","execution_count":2,"id":"d9adb5d1","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:25.147649Z","iopub.status.busy":"2023-12-23T04:24:25.147248Z","iopub.status.idle":"2023-12-23T04:24:25.178043Z","shell.execute_reply":"2023-12-23T04:24:25.176522Z"},"papermill":{"duration":0.043793,"end_time":"2023-12-23T04:24:25.180628","exception":false,"start_time":"2023-12-23T04:24:25.136835","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7905 entries, 0 to 7904\n","Data columns (total 19 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   N_Days         7905 non-null   int64  \n"," 1   Drug           7905 non-null   object \n"," 2   Age            7905 non-null   int64  \n"," 3   Sex            7905 non-null   object \n"," 4   Ascites        7905 non-null   object \n"," 5   Hepatomegaly   7905 non-null   object \n"," 6   Spiders        7905 non-null   object \n"," 7   Edema          7905 non-null   object \n"," 8   Bilirubin      7905 non-null   float64\n"," 9   Cholesterol    7905 non-null   float64\n"," 10  Albumin        7905 non-null   float64\n"," 11  Copper         7905 non-null   float64\n"," 12  Alk_Phos       7905 non-null   float64\n"," 13  SGOT           7905 non-null   float64\n"," 14  Tryglicerides  7905 non-null   float64\n"," 15  Platelets      7905 non-null   float64\n"," 16  Prothrombin    7905 non-null   float64\n"," 17  Stage          7905 non-null   float64\n"," 18  Status         7905 non-null   object \n","dtypes: float64(10), int64(2), object(7)\n","memory usage: 1.1+ MB\n"]}],"source":["# Use info to see the number of categorical columns (any column with Dtype=object is text and will need to be converted to a number) \n","train_df.info()\n","\n","# Make a list of categorical columns for future use (in the feature scaling section)\n","categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']"]},{"cell_type":"code","execution_count":3,"id":"1f821f1e","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:25.200787Z","iopub.status.busy":"2023-12-23T04:24:25.200345Z","iopub.status.idle":"2023-12-23T04:24:25.254202Z","shell.execute_reply":"2023-12-23T04:24:25.253318Z"},"papermill":{"duration":0.066464,"end_time":"2023-12-23T04:24:25.256356","exception":false,"start_time":"2023-12-23T04:24:25.189892","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2030.173308</td>\n","      <td>18373.146490</td>\n","      <td>2.594485</td>\n","      <td>350.561923</td>\n","      <td>3.548323</td>\n","      <td>83.902846</td>\n","      <td>1816.745250</td>\n","      <td>114.604602</td>\n","      <td>115.340164</td>\n","      <td>265.228969</td>\n","      <td>10.629462</td>\n","      <td>3.032511</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1094.233744</td>\n","      <td>3679.958739</td>\n","      <td>3.812960</td>\n","      <td>195.379344</td>\n","      <td>0.346171</td>\n","      <td>75.899266</td>\n","      <td>1903.750657</td>\n","      <td>48.790945</td>\n","      <td>52.530402</td>\n","      <td>87.465579</td>\n","      <td>0.781735</td>\n","      <td>0.866511</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>41.000000</td>\n","      <td>9598.000000</td>\n","      <td>0.300000</td>\n","      <td>120.000000</td>\n","      <td>1.960000</td>\n","      <td>4.000000</td>\n","      <td>289.000000</td>\n","      <td>26.350000</td>\n","      <td>33.000000</td>\n","      <td>62.000000</td>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1230.000000</td>\n","      <td>15574.000000</td>\n","      <td>0.700000</td>\n","      <td>248.000000</td>\n","      <td>3.350000</td>\n","      <td>39.000000</td>\n","      <td>834.000000</td>\n","      <td>75.950000</td>\n","      <td>84.000000</td>\n","      <td>211.000000</td>\n","      <td>10.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1831.000000</td>\n","      <td>18713.000000</td>\n","      <td>1.100000</td>\n","      <td>298.000000</td>\n","      <td>3.580000</td>\n","      <td>63.000000</td>\n","      <td>1181.000000</td>\n","      <td>108.500000</td>\n","      <td>104.000000</td>\n","      <td>265.000000</td>\n","      <td>10.600000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2689.000000</td>\n","      <td>20684.000000</td>\n","      <td>3.000000</td>\n","      <td>390.000000</td>\n","      <td>3.770000</td>\n","      <td>102.000000</td>\n","      <td>1857.000000</td>\n","      <td>137.950000</td>\n","      <td>139.000000</td>\n","      <td>316.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4795.000000</td>\n","      <td>28650.000000</td>\n","      <td>28.000000</td>\n","      <td>1775.000000</td>\n","      <td>4.640000</td>\n","      <td>588.000000</td>\n","      <td>13862.400000</td>\n","      <td>457.250000</td>\n","      <td>598.000000</td>\n","      <td>563.000000</td>\n","      <td>18.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days           Age    Bilirubin  Cholesterol      Albumin  \\\n","count  7905.000000   7905.000000  7905.000000  7905.000000  7905.000000   \n","mean   2030.173308  18373.146490     2.594485   350.561923     3.548323   \n","std    1094.233744   3679.958739     3.812960   195.379344     0.346171   \n","min      41.000000   9598.000000     0.300000   120.000000     1.960000   \n","25%    1230.000000  15574.000000     0.700000   248.000000     3.350000   \n","50%    1831.000000  18713.000000     1.100000   298.000000     3.580000   \n","75%    2689.000000  20684.000000     3.000000   390.000000     3.770000   \n","max    4795.000000  28650.000000    28.000000  1775.000000     4.640000   \n","\n","            Copper      Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  7905.000000   7905.000000  7905.000000    7905.000000  7905.000000   \n","mean     83.902846   1816.745250   114.604602     115.340164   265.228969   \n","std      75.899266   1903.750657    48.790945      52.530402    87.465579   \n","min       4.000000    289.000000    26.350000      33.000000    62.000000   \n","25%      39.000000    834.000000    75.950000      84.000000   211.000000   \n","50%      63.000000   1181.000000   108.500000     104.000000   265.000000   \n","75%     102.000000   1857.000000   137.950000     139.000000   316.000000   \n","max     588.000000  13862.400000   457.250000     598.000000   563.000000   \n","\n","       Prothrombin        Stage  \n","count  7905.000000  7905.000000  \n","mean     10.629462     3.032511  \n","std       0.781735     0.866511  \n","min       9.000000     1.000000  \n","25%      10.000000     2.000000  \n","50%      10.600000     3.000000  \n","75%      11.000000     4.000000  \n","max      18.000000     4.000000  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Use the describe method to generate statistical information such as standard deviation, mean, and min/max. \n","\n","train_df.describe()"]},{"cell_type":"markdown","id":"b38b2b28","metadata":{"papermill":{"duration":0.00912,"end_time":"2023-12-23T04:24:25.275052","exception":false,"start_time":"2023-12-23T04:24:25.265932","status":"completed"},"tags":[]},"source":["## Dataset Pre-Processing Task Overview\n","\n","1. Target Variable Label Encoding \n","2. Missing Feature Value Handling \n","3. Feature Scaling \n","4. Categorical Attribute Handling (Should be done at the end since it may change the number of columns)\n","5. Train/Test Split"]},{"cell_type":"markdown","id":"b971574f","metadata":{"papermill":{"duration":0.009114,"end_time":"2023-12-23T04:24:25.29363","exception":false,"start_time":"2023-12-23T04:24:25.284516","status":"completed"},"tags":[]},"source":["# Target Variable Label Encoding\n","\n","Problem: Since the target column, Status, is a text column, we need to convert the column's values into integers so the machine learning model can process it.\n","\n","Possible Approaches:\n","1. Label Encoding - Convert each text category into an integer label. \n","2. Ordinal Encoding - Convert each text category into an integer label but with a particular order. Used when the categories have some quantitative order that can be taken advantage of like low, medium, high.   \n","3. One-Hot Encoding - Convert each text category into a separate column. For example, this is done in the softmax layer of a neural network. \n","\n","Chosen Approach: Label Encoding - There is no obvious ordering in the 'Status' column and ML libraries typically expect a single target column so we will use label encoding."]},{"cell_type":"code","execution_count":4,"id":"e84980ea","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:25.314731Z","iopub.status.busy":"2023-12-23T04:24:25.313762Z","iopub.status.idle":"2023-12-23T04:24:25.323164Z","shell.execute_reply":"2023-12-23T04:24:25.322334Z"},"papermill":{"duration":0.022174,"end_time":"2023-12-23T04:24:25.325271","exception":false,"start_time":"2023-12-23T04:24:25.303097","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","C     4965\n","D     2665\n","CL     275\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df['Status'].value_counts()"]},{"cell_type":"code","execution_count":5,"id":"1c551a31","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:25.346586Z","iopub.status.busy":"2023-12-23T04:24:25.345929Z","iopub.status.idle":"2023-12-23T04:24:26.511369Z","shell.execute_reply":"2023-12-23T04:24:26.510573Z"},"papermill":{"duration":1.178604,"end_time":"2023-12-23T04:24:26.513482","exception":false,"start_time":"2023-12-23T04:24:25.334878","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","0    4965\n","2    2665\n","1     275\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Use sklearn's LabelEncoder class to transform the Status column's values from strings to integers.\n","label_encoder = LabelEncoder() \n","train_df['Status'] = label_encoder.fit_transform(train_df['Status'])\n","\n","# Check that the label encoder transformation was applied correctly\n","train_df['Status'].value_counts()"]},{"cell_type":"markdown","id":"71e431ab","metadata":{"papermill":{"duration":0.00969,"end_time":"2023-12-23T04:24:26.532948","exception":false,"start_time":"2023-12-23T04:24:26.523258","status":"completed"},"tags":[]},"source":["## Missing Feature Value Handling\n","\n","Problem: We need to check if there are any missing feature values since most machine learning algorithms cannot handle an empty cell. For example, a Logistic Regression model would throw an error in training although other algorithms like XGBoost can train normally by substituting in a value as needed. \n","\n","Solution: We check for missing feature values by using the isna DataFrame method which returns a boolean DataFrame where each cell is True if the value is missing and False otherwise. We then apply the sum method to find the number of missing values in each column. "]},{"cell_type":"code","execution_count":6,"id":"d5062c61","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.554603Z","iopub.status.busy":"2023-12-23T04:24:26.55365Z","iopub.status.idle":"2023-12-23T04:24:26.567328Z","shell.execute_reply":"2023-12-23T04:24:26.565705Z"},"papermill":{"duration":0.027007,"end_time":"2023-12-23T04:24:26.56965","exception":false,"start_time":"2023-12-23T04:24:26.542643","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column: \n","N_Days           0\n","Drug             0\n","Age              0\n","Sex              0\n","Ascites          0\n","Hepatomegaly     0\n","Spiders          0\n","Edema            0\n","Bilirubin        0\n","Cholesterol      0\n","Albumin          0\n","Copper           0\n","Alk_Phos         0\n","SGOT             0\n","Tryglicerides    0\n","Platelets        0\n","Prothrombin      0\n","Stage            0\n","Status           0\n","dtype: int64\n"]}],"source":["# Check for missing feature values\n","print(\"Number of missing feature values by column: \")\n","print(train_df.isna().sum())"]},{"cell_type":"code","execution_count":7,"id":"f2fe7864","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.591996Z","iopub.status.busy":"2023-12-23T04:24:26.5909Z","iopub.status.idle":"2023-12-23T04:24:26.601895Z","shell.execute_reply":"2023-12-23T04:24:26.600372Z"},"papermill":{"duration":0.024593,"end_time":"2023-12-23T04:24:26.604161","exception":false,"start_time":"2023-12-23T04:24:26.579568","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column in test data: 0\n"]}],"source":["# Alternatively we can check the total number of missing feature values in the dataset this way. \n","print(\"Number of missing feature values by column in test data:\", test_df.isna().sum().sum())"]},{"cell_type":"markdown","id":"8d29157c","metadata":{"papermill":{"duration":0.009848,"end_time":"2023-12-23T04:24:26.62405","exception":false,"start_time":"2023-12-23T04:24:26.614202","status":"completed"},"tags":[]},"source":["## Feature Scaling\n","\n","Problem: Typically since feature column values are combined (often by adding them together) to create the final classification, the ML model will perform better if the features are on the same scale. \n","\n","Possible Feature Scaling Approaches: \n","1. Min-Max Scaling - Scales the data to a fixed range between two values (typically 0 and 1). Most useful for neural networks.\n","2. Standardization (Z-score Normalization) - Scales the data so that the mean is 0 and the standard deviation is 1. Most useful for algorithms that assume a normal distribution of data, such as SVMs and logistic regression.\n","3. Robust Scaling - Scaling based on median and IQR. Most useful for handling significant outliers.\n","4. MaxAbsScaler - Scales each feature based on its maximum absolute value. Useful for sparse data. \n","\n","Chosen Approach: Standardization - Since XGBoost's decision tree classification uses splitting which occurs within a column, different column values do not interact with each other and therefore scaling the features is not necessary. However, since we are using Logistic Regression as our baseline model and features do interact in training (they add together to determine the model's class prediction) we will need to do feature scaling (you don't want one column to be in the millions and the other to be in the single digits). Since we are doing feature scaling specifically for our logistic regression model we will use the Standardization approach. "]},{"cell_type":"code","execution_count":8,"id":"8722dcb6","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.646306Z","iopub.status.busy":"2023-12-23T04:24:26.645882Z","iopub.status.idle":"2023-12-23T04:24:26.714019Z","shell.execute_reply":"2023-12-23T04:24:26.712921Z"},"papermill":{"duration":0.082386,"end_time":"2023-12-23T04:24:26.716537","exception":false,"start_time":"2023-12-23T04:24:26.634151","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.007790</td>\n","      <td>0.033864</td>\n","      <td>0.001549</td>\n","      <td>0.009851</td>\n","      <td>-0.029617</td>\n","      <td>0.010526</td>\n","      <td>-0.002895</td>\n","      <td>-0.020847</td>\n","      <td>-0.001029</td>\n","      <td>-0.013781</td>\n","      <td>0.004353</td>\n","      <td>0.005175</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.993309</td>\n","      <td>0.973958</td>\n","      <td>1.010406</td>\n","      <td>1.025961</td>\n","      <td>1.025240</td>\n","      <td>1.021709</td>\n","      <td>1.016664</td>\n","      <td>1.003627</td>\n","      <td>1.001441</td>\n","      <td>1.001418</td>\n","      <td>1.014105</td>\n","      <td>0.987967</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-1.817984</td>\n","      <td>-2.384728</td>\n","      <td>-0.601797</td>\n","      <td>-1.180148</td>\n","      <td>-4.588553</td>\n","      <td>-1.052815</td>\n","      <td>-0.802543</td>\n","      <td>-1.808946</td>\n","      <td>-1.567576</td>\n","      <td>-2.323678</td>\n","      <td>-2.084550</td>\n","      <td>-2.345776</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-0.727654</td>\n","      <td>-0.718300</td>\n","      <td>-0.496885</td>\n","      <td>-0.524971</td>\n","      <td>-0.572940</td>\n","      <td>-0.591648</td>\n","      <td>-0.522026</td>\n","      <td>-0.811772</td>\n","      <td>-0.596648</td>\n","      <td>-0.642910</td>\n","      <td>-0.805263</td>\n","      <td>-1.191649</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>-0.135421</td>\n","      <td>0.117632</td>\n","      <td>-0.391973</td>\n","      <td>-0.263923</td>\n","      <td>0.062625</td>\n","      <td>-0.249068</td>\n","      <td>-0.354452</td>\n","      <td>-0.156896</td>\n","      <td>-0.215892</td>\n","      <td>-0.071221</td>\n","      <td>-0.037691</td>\n","      <td>-0.037522</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.604869</td>\n","      <td>0.627996</td>\n","      <td>0.106359</td>\n","      <td>0.201867</td>\n","      <td>0.640411</td>\n","      <td>0.238452</td>\n","      <td>0.011428</td>\n","      <td>0.478508</td>\n","      <td>0.431393</td>\n","      <td>0.591939</td>\n","      <td>0.474024</td>\n","      <td>1.116605</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2.526884</td>\n","      <td>2.792831</td>\n","      <td>6.663359</td>\n","      <td>7.291089</td>\n","      <td>3.153780</td>\n","      <td>6.642081</td>\n","      <td>6.327728</td>\n","      <td>7.023169</td>\n","      <td>9.188781</td>\n","      <td>3.404652</td>\n","      <td>5.847030</td>\n","      <td>1.116605</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days          Age    Bilirubin  Cholesterol      Albumin  \\\n","count  5271.000000  5271.000000  5271.000000  5271.000000  5271.000000   \n","mean      0.007790     0.033864     0.001549     0.009851    -0.029617   \n","std       0.993309     0.973958     1.010406     1.025961     1.025240   \n","min      -1.817984    -2.384728    -0.601797    -1.180148    -4.588553   \n","25%      -0.727654    -0.718300    -0.496885    -0.524971    -0.572940   \n","50%      -0.135421     0.117632    -0.391973    -0.263923     0.062625   \n","75%       0.604869     0.627996     0.106359     0.201867     0.640411   \n","max       2.526884     2.792831     6.663359     7.291089     3.153780   \n","\n","            Copper     Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  5271.000000  5271.000000  5271.000000    5271.000000  5271.000000   \n","mean      0.010526    -0.002895    -0.020847      -0.001029    -0.013781   \n","std       1.021709     1.016664     1.003627       1.001441     1.001418   \n","min      -1.052815    -0.802543    -1.808946      -1.567576    -2.323678   \n","25%      -0.591648    -0.522026    -0.811772      -0.596648    -0.642910   \n","50%      -0.249068    -0.354452    -0.156896      -0.215892    -0.071221   \n","75%       0.238452     0.011428     0.478508       0.431393     0.591939   \n","max       6.642081     6.327728     7.023169       9.188781     3.404652   \n","\n","       Prothrombin        Stage  \n","count  5271.000000  5271.000000  \n","mean      0.004353     0.005175  \n","std       1.014105     0.987967  \n","min      -2.084550    -2.345776  \n","25%      -0.805263    -1.191649  \n","50%      -0.037691    -0.037522  \n","75%       0.474024     1.116605  \n","max       5.847030     1.116605  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler \n","\n","numerical_cols = train_df.columns.difference(categorical_cols)\n","\n","std_scaler = StandardScaler() \n","std_scaler.fit(train_df[numerical_cols])\n","\n","train_df[numerical_cols] = std_scaler.transform(train_df[numerical_cols])\n","test_df[numerical_cols] = std_scaler.transform(test_df[numerical_cols])\n","                      \n","# Confirm the transformation was successful by seeing if the mean = 0 and std = 1 for numerical columns\n","test_df.describe()"]},{"cell_type":"markdown","id":"5968063b","metadata":{"papermill":{"duration":0.01017,"end_time":"2023-12-23T04:24:26.737327","exception":false,"start_time":"2023-12-23T04:24:26.727157","status":"completed"},"tags":[]},"source":["## Handling Categorical Attributes/Columns \n","Problem: We use the DataFrame info method to find that there are six categorical columns: drug, sex, ascites, hepatomegaly, spiders, edema which we need to convert from text into numerical values since machine learning models cannot handle text data naturally, they can only handle numbers.\n","\n","Possible Approaches: The main approaches for categorical attribute handling are \n","1. Ordinal Encoding - Useful when the categories correspond to an ascending or descending order. \n","2. One-Hot Encoding - For each categorical column, convert it into multiple columns, one for each possible category. This is used when the categories do not have an obvious logical order. \n","3. Numerical Feature Replacement (Advanced) - In cases where the number of categories is cery large (hundreds or thousands) one should consider replacing the categorical columns with a numerical column that converts each category into some number. For example, one could convert a country code into the country's population. \n","4. Embedding Replacement (Advanced) - Alternatively, one can replace categories with embeddings, which are low dimensional vectors that represent the category. \n","\n","Chosen Approach : One-Hot Encoding - In this case, we use a one-hot encoding since none of the categories seem to have an order to the classes (ruling out ordinal encoding) and the number of categories for each column is low (under 10 for all categorical columns) which rules out needing advanced methods. "]},{"cell_type":"code","execution_count":9,"id":"7b6bb778","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.760536Z","iopub.status.busy":"2023-12-23T04:24:26.760077Z","iopub.status.idle":"2023-12-23T04:24:26.776103Z","shell.execute_reply":"2023-12-23T04:24:26.774952Z"},"papermill":{"duration":0.030368,"end_time":"2023-12-23T04:24:26.778268","exception":false,"start_time":"2023-12-23T04:24:26.7479","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["N_Days           461\n","Drug               2\n","Age              391\n","Sex                2\n","Ascites            2\n","Hepatomegaly       2\n","Spiders            2\n","Edema              3\n","Bilirubin        111\n","Cholesterol      226\n","Albumin          160\n","Copper           171\n","Alk_Phos         364\n","SGOT             206\n","Tryglicerides    154\n","Platelets        227\n","Prothrombin       49\n","Stage              4\n","Status             3\n","dtype: int64\n"]}],"source":["# Confirm that the number of categories in the categorical columns is manageable (< 100)\n","unique_values_per_column = train_df.nunique()\n","\n","print(unique_values_per_column)"]},{"cell_type":"code","execution_count":10,"id":"91204961","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.800983Z","iopub.status.busy":"2023-12-23T04:24:26.800285Z","iopub.status.idle":"2023-12-23T04:24:26.842929Z","shell.execute_reply":"2023-12-23T04:24:26.84142Z"},"papermill":{"duration":0.056641,"end_time":"2023-12-23T04:24:26.845414","exception":false,"start_time":"2023-12-23T04:24:26.788773","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5271 entries, 0 to 5270\n","Data columns (total 25 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   N_Days                5271 non-null   float64\n"," 1   Age                   5271 non-null   float64\n"," 2   Bilirubin             5271 non-null   float64\n"," 3   Cholesterol           5271 non-null   float64\n"," 4   Albumin               5271 non-null   float64\n"," 5   Copper                5271 non-null   float64\n"," 6   Alk_Phos              5271 non-null   float64\n"," 7   SGOT                  5271 non-null   float64\n"," 8   Tryglicerides         5271 non-null   float64\n"," 9   Platelets             5271 non-null   float64\n"," 10  Prothrombin           5271 non-null   float64\n"," 11  Stage                 5271 non-null   float64\n"," 12  Drug_D-penicillamine  5271 non-null   bool   \n"," 13  Drug_Placebo          5271 non-null   bool   \n"," 14  Sex_F                 5271 non-null   bool   \n"," 15  Sex_M                 5271 non-null   bool   \n"," 16  Ascites_N             5271 non-null   bool   \n"," 17  Ascites_Y             5271 non-null   bool   \n"," 18  Hepatomegaly_N        5271 non-null   bool   \n"," 19  Hepatomegaly_Y        5271 non-null   bool   \n"," 20  Spiders_N             5271 non-null   bool   \n"," 21  Spiders_Y             5271 non-null   bool   \n"," 22  Edema_N               5271 non-null   bool   \n"," 23  Edema_S               5271 non-null   bool   \n"," 24  Edema_Y               5271 non-null   bool   \n","dtypes: bool(13), float64(12)\n","memory usage: 561.2 KB\n"]}],"source":["# Convert the categorical columns into one-hot encodings\n","status = train_df['Status']\n","train_df_dummies = pd.get_dummies(train_df.drop('Status', axis=1))\n","train_df = pd.concat([train_df_dummies, status], axis=1)\n","test_df = pd.get_dummies(test_df)\n","\n","# Confirm the transformation was successful\n","test_df.info()"]},{"cell_type":"markdown","id":"6598cf43","metadata":{"papermill":{"duration":0.010348,"end_time":"2023-12-23T04:24:26.868497","exception":false,"start_time":"2023-12-23T04:24:26.858149","status":"completed"},"tags":[]},"source":["# Train/Test Split\n","\n","Problem: We need to split the dataset into a training dataset and a testing/tuning dataset.\n","\n","Chosen Approach : Sklearn Library - We wait until all dataset pre-processing is done (to avoid needing to pre-process the train/test datasets separately) and then split the dataset using sklearn's train_test_split function. A typical split between train and test set is 80%/20% which we specify with the test_size = 0.2 argument. "]},{"cell_type":"code","execution_count":11,"id":"8080b8c6","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:26.891547Z","iopub.status.busy":"2023-12-23T04:24:26.890747Z","iopub.status.idle":"2023-12-23T04:24:26.994863Z","shell.execute_reply":"2023-12-23T04:24:26.993754Z"},"papermill":{"duration":0.118639,"end_time":"2023-12-23T04:24:26.997597","exception":false,"start_time":"2023-12-23T04:24:26.878958","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the training set into a training and test set \n","X = train_df.drop(\"Status\", axis=1)\n","y = train_df[\"Status\"]\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size = 0.2)"]},{"cell_type":"markdown","id":"e3f277e6","metadata":{"papermill":{"duration":0.010355,"end_time":"2023-12-23T04:24:27.018795","exception":false,"start_time":"2023-12-23T04:24:27.00844","status":"completed"},"tags":[]},"source":["# Logistic Regression Baseline Model"]},{"cell_type":"code","execution_count":12,"id":"12b3a821","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:27.042271Z","iopub.status.busy":"2023-12-23T04:24:27.041178Z","iopub.status.idle":"2023-12-23T04:24:27.64437Z","shell.execute_reply":"2023-12-23T04:24:27.642958Z"},"papermill":{"duration":0.619521,"end_time":"2023-12-23T04:24:27.648833","exception":false,"start_time":"2023-12-23T04:24:27.029312","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.82321794 0.01325625 0.16352581]\n"," [0.87794609 0.04462442 0.07742949]\n"," [0.04035548 0.01217423 0.94747029]\n"," ...\n"," [0.81433742 0.02321217 0.16245042]\n"," [0.95132835 0.02966722 0.01900442]\n"," [0.31038946 0.01086457 0.67874597]]\n"]}],"source":["from sklearn.linear_model import LogisticRegression \n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Train the model using sci-kit learn's Logistic Regression model \n","model = LogisticRegression(max_iter = 1000)\n","# The fit method learns the parameters (weights) for the model\n","model.fit(X_train, y_train)\n","# The predict_proba method predicts a probability for each of the three possible classes for each patient (row in the dataset)\n","y_pred_lr = model.predict_proba(test_df)\n","\n","print(y_pred_lr)\n","\n","# Evaluate the results\n","#print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","#print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","#print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"]},{"cell_type":"markdown","id":"b947517b","metadata":{"papermill":{"duration":0.02272,"end_time":"2023-12-23T04:24:27.69449","exception":false,"start_time":"2023-12-23T04:24:27.67177","status":"completed"},"tags":[]},"source":["# Random Forest Model Training"]},{"cell_type":"code","execution_count":13,"id":"58776062","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:27.742973Z","iopub.status.busy":"2023-12-23T04:24:27.742254Z","iopub.status.idle":"2023-12-23T04:24:29.464866Z","shell.execute_reply":"2023-12-23T04:24:29.463673Z"},"papermill":{"duration":1.750484,"end_time":"2023-12-23T04:24:29.467737","exception":false,"start_time":"2023-12-23T04:24:27.717253","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.63 0.08 0.29]\n"," [0.69 0.14 0.17]\n"," [0.09 0.06 0.85]\n"," ...\n"," [0.96 0.   0.04]\n"," [0.96 0.01 0.03]\n"," [0.41 0.06 0.53]]\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Train the model using sci-kit learn's Random Forest model \n","rf_classifier = RandomForestClassifier() \n","# The fit method learns the parameters (weights) for the model\n","rf_classifier.fit(X_train, y_train) \n","# The predict_proba method predicts a probability for each of the three possible classes for each patient (row in the dataset)\n","y_pred_rf = rf_classifier.predict_proba(test_df) \n","\n","print(y_pred_rf)"]},{"cell_type":"markdown","id":"96372892","metadata":{"papermill":{"duration":0.010752,"end_time":"2023-12-23T04:24:29.491209","exception":false,"start_time":"2023-12-23T04:24:29.480457","status":"completed"},"tags":[]},"source":["# XGBoost Model Training \n"]},{"cell_type":"code","execution_count":14,"id":"54d59d9e","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:29.515007Z","iopub.status.busy":"2023-12-23T04:24:29.514594Z","iopub.status.idle":"2023-12-23T04:24:30.067725Z","shell.execute_reply":"2023-12-23T04:24:30.066777Z"},"papermill":{"duration":0.568166,"end_time":"2023-12-23T04:24:30.070311","exception":false,"start_time":"2023-12-23T04:24:29.502145","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.57985884 0.03010576 0.39003536]\n"," [0.3412138  0.5642541  0.09453212]\n"," [0.01249615 0.0052208  0.98228306]\n"," ...\n"," [0.74064523 0.02396731 0.23538741]\n"," [0.9900164  0.00335872 0.00662492]\n"," [0.35335645 0.00419964 0.6424439 ]]\n"]}],"source":["from xgboost import XGBClassifier \n","\n","# Train the model using xgboost's XGBClassifier model \n","xg_model = XGBClassifier(n_estimators=100, max_depth=2) \n","# The fit method learns the parameters (weights) for the model\n","xg_model.fit(X_train, y_train)\n","# The predict_proba method predicts a probability for each of the three possible classes for each patient (row in the dataset)\n","y_pred_xg = xg_model.predict_proba(test_df)\n","\n","print(y_pred_xg)"]},{"cell_type":"markdown","id":"b5b331a2","metadata":{"papermill":{"duration":0.010702,"end_time":"2023-12-23T04:24:30.093176","exception":false,"start_time":"2023-12-23T04:24:30.082474","status":"completed"},"tags":[]},"source":["# Ensemble Training"]},{"cell_type":"code","execution_count":15,"id":"47fdcbd3","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:30.117186Z","iopub.status.busy":"2023-12-23T04:24:30.116785Z","iopub.status.idle":"2023-12-23T04:24:30.121984Z","shell.execute_reply":"2023-12-23T04:24:30.120768Z"},"papermill":{"duration":0.020077,"end_time":"2023-12-23T04:24:30.12446","exception":false,"start_time":"2023-12-23T04:24:30.104383","status":"completed"},"tags":[]},"outputs":[],"source":["# Ensemble training"]},{"cell_type":"markdown","id":"84d30a5f","metadata":{"papermill":{"duration":0.010729,"end_time":"2023-12-23T04:24:30.146512","exception":false,"start_time":"2023-12-23T04:24:30.135783","status":"completed"},"tags":[]},"source":["# Kaggle Submission Processing\n","1. Create a submission dataframe from the model's predictions \n","2. Concatenate the data id column values to adhere to submission formatting requirements\n","3. Convert the submission dataframe into a csv file for submission \n","4. Now in order to submit to Kaggle, save the notebook and navigate to the Submissions page for this competition and click 'Submit Prediction' in the top-right corner -> Notebook -> Submit. "]},{"cell_type":"code","execution_count":16,"id":"466912d7","metadata":{"execution":{"iopub.execute_input":"2023-12-23T04:24:30.170707Z","iopub.status.busy":"2023-12-23T04:24:30.170089Z","iopub.status.idle":"2023-12-23T04:24:30.214195Z","shell.execute_reply":"2023-12-23T04:24:30.213061Z"},"papermill":{"duration":0.059435,"end_time":"2023-12-23T04:24:30.216952","exception":false,"start_time":"2023-12-23T04:24:30.157517","status":"completed"},"tags":[]},"outputs":[],"source":["# Modify the probability predictions into the submission format \n","submission_df = pd.DataFrame(y_pred_xg, columns=['Status_C', 'Status_CL', 'Status_D'])\n","final_submission_df = pd.concat([test_id_df, submission_df], axis=1)\n","final_submission_df.head(10)\n","\n","# Create a submission.csv file that Kaggle will automatically evaluate for submission\n","final_submission_df.to_csv('submission.csv', index = False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7000181,"sourceId":60893,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":9.742596,"end_time":"2023-12-23T04:24:30.849384","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-23T04:24:21.106788","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}