{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jonathankao/dec-2023-tabular-ensemble-rf-xgboost-lgbm?scriptVersionId=157158138\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ccc7364a","metadata":{"papermill":{"duration":0.010719,"end_time":"2023-12-31T01:35:52.007319","exception":false,"start_time":"2023-12-31T01:35:51.9966","status":"completed"},"tags":[]},"source":["# Task Overview \n","Task: In this synthetic dataset based off of a real dataset funded by the Mayo Clinic, each example represents both general and survival information about a patient that has liver cirrhosis, a condition involving prolonged liver damage. The goal is to train a machine learning model that can predict the patient's current survival status based on the data features. \n","\n","Approach v1.0: Our approach will be to train a Logistic Regression model using sklearn to be used as a baseilne model and to then train a performance-focused model using XGBoost as a learning exercise.\n","\n","Approach v2.0: After a few days of initial work, we realized that it was feasible within the competition timeframe to improve on Approach 1 by training an Ensemble of models so we changed our approach to training multiple Logistic Regression, Random Forest, and XGBoost models and then combining them into an ensemble.\n","\n","Approach v3.0: After manual hyperparameter tuning all the models, we realized that the current Logistic Regression and Random Forest models were too weak to be implemented in the final Ensemble model. Instead we decided to add a LGBM classifier and create an Ensemble model with a LGBM classifier + XGBoost classifier. \n","\n","Next Steps: Try to implement more robust automated hyperparameter tuning so that we can add additional storng-performing models to the Ensemble. \n","\n","Version History\n","1. v1.0-1.3 - Implemented dataset loading and dataset pre-processing. Added explanations and comments for each step. \n","2. v1.4 - Added Logistic Regression model training and Kaggle submission formatting\n","3. v2.0 - Added Random Forest and XGBoost model training and prediction.\n","4. v2.1 - Added log-loss calculations for each model on the validation set. \n","5. v2.2 - Added initial hyperparameter tuning for Logistic Regression, Random Forest, and XGBoost models. \n","6. v3.0 - Added LGBM Classifier and did hyperparameter tuning for the LGBM Classifier. Added explanations for model training / tuning\n","7. v3.1 - Combined the tuned LGBM Classifier and XGBoost classifer into an Ensemble model. \n"]},{"cell_type":"markdown","id":"1899310c","metadata":{"papermill":{"duration":0.009382,"end_time":"2023-12-31T01:35:52.026587","exception":false,"start_time":"2023-12-31T01:35:52.017205","status":"completed"},"tags":[]},"source":["# Dataset Loading"]},{"cell_type":"code","execution_count":1,"id":"78f6740f","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:52.048326Z","iopub.status.busy":"2023-12-31T01:35:52.047878Z","iopub.status.idle":"2023-12-31T01:35:52.617717Z","shell.execute_reply":"2023-12-31T01:35:52.616584Z"},"papermill":{"duration":0.584106,"end_time":"2023-12-31T01:35:52.620471","exception":false,"start_time":"2023-12-31T01:35:52.036365","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Drug</th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Ascites</th>\n","      <th>Hepatomegaly</th>\n","      <th>Spiders</th>\n","      <th>Edema</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>999</td>\n","      <td>D-penicillamine</td>\n","      <td>21532</td>\n","      <td>M</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>2.3</td>\n","      <td>316.0</td>\n","      <td>3.35</td>\n","      <td>172.0</td>\n","      <td>1601.0</td>\n","      <td>179.80</td>\n","      <td>63.0</td>\n","      <td>394.0</td>\n","      <td>9.7</td>\n","      <td>3.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2574</td>\n","      <td>Placebo</td>\n","      <td>19237</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.9</td>\n","      <td>364.0</td>\n","      <td>3.54</td>\n","      <td>63.0</td>\n","      <td>1440.0</td>\n","      <td>134.85</td>\n","      <td>88.0</td>\n","      <td>361.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3428</td>\n","      <td>Placebo</td>\n","      <td>13727</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>3.3</td>\n","      <td>299.0</td>\n","      <td>3.55</td>\n","      <td>131.0</td>\n","      <td>1029.0</td>\n","      <td>119.35</td>\n","      <td>50.0</td>\n","      <td>199.0</td>\n","      <td>11.7</td>\n","      <td>4.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2576</td>\n","      <td>Placebo</td>\n","      <td>18460</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.6</td>\n","      <td>256.0</td>\n","      <td>3.50</td>\n","      <td>58.0</td>\n","      <td>1653.0</td>\n","      <td>71.30</td>\n","      <td>96.0</td>\n","      <td>269.0</td>\n","      <td>10.7</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>788</td>\n","      <td>Placebo</td>\n","      <td>16658</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>1.1</td>\n","      <td>346.0</td>\n","      <td>3.65</td>\n","      <td>63.0</td>\n","      <td>1181.0</td>\n","      <td>125.55</td>\n","      <td>96.0</td>\n","      <td>298.0</td>\n","      <td>10.6</td>\n","      <td>4.0</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n","0     999  D-penicillamine  21532   M       N            N       N     N   \n","1    2574          Placebo  19237   F       N            N       N     N   \n","2    3428          Placebo  13727   F       N            Y       Y     Y   \n","3    2576          Placebo  18460   F       N            N       N     N   \n","4     788          Placebo  16658   F       N            Y       N     N   \n","\n","   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n","0        2.3        316.0     3.35   172.0    1601.0  179.80           63.0   \n","1        0.9        364.0     3.54    63.0    1440.0  134.85           88.0   \n","2        3.3        299.0     3.55   131.0    1029.0  119.35           50.0   \n","3        0.6        256.0     3.50    58.0    1653.0   71.30           96.0   \n","4        1.1        346.0     3.65    63.0    1181.0  125.55           96.0   \n","\n","   Platelets  Prothrombin  Stage Status  \n","0      394.0          9.7    3.0      D  \n","1      361.0         11.0    3.0      C  \n","2      199.0         11.7    4.0      D  \n","3      269.0         10.7    3.0      C  \n","4      298.0         10.6    4.0      C  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Read in the initial competition training and test data as Pandas DataFrames \n","train_file_path = \"/kaggle/input/playground-series-s3e26/train.csv\"\n","train_df = pd.read_csv(train_file_path) \n","test_file_path = \"/kaggle/input/playground-series-s3e26/test.csv\"\n","test_df = pd.read_csv(test_file_path)\n","\n","# Remove the id column since it is not useful for prediction and might confuse the model in training\n","test_id_df = test_df['id'].astype(int)\n","train_df = train_df.drop('id', axis=1)\n","test_df = test_df.drop('id', axis=1)\n","\n","# Use the head method to visually see that the dataset has been loaded\n","train_df.head(5)"]},{"cell_type":"code","execution_count":2,"id":"20a0c2ff","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:52.643567Z","iopub.status.busy":"2023-12-31T01:35:52.643169Z","iopub.status.idle":"2023-12-31T01:35:52.675518Z","shell.execute_reply":"2023-12-31T01:35:52.674042Z"},"papermill":{"duration":0.047035,"end_time":"2023-12-31T01:35:52.678003","exception":false,"start_time":"2023-12-31T01:35:52.630968","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7905 entries, 0 to 7904\n","Data columns (total 19 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   N_Days         7905 non-null   int64  \n"," 1   Drug           7905 non-null   object \n"," 2   Age            7905 non-null   int64  \n"," 3   Sex            7905 non-null   object \n"," 4   Ascites        7905 non-null   object \n"," 5   Hepatomegaly   7905 non-null   object \n"," 6   Spiders        7905 non-null   object \n"," 7   Edema          7905 non-null   object \n"," 8   Bilirubin      7905 non-null   float64\n"," 9   Cholesterol    7905 non-null   float64\n"," 10  Albumin        7905 non-null   float64\n"," 11  Copper         7905 non-null   float64\n"," 12  Alk_Phos       7905 non-null   float64\n"," 13  SGOT           7905 non-null   float64\n"," 14  Tryglicerides  7905 non-null   float64\n"," 15  Platelets      7905 non-null   float64\n"," 16  Prothrombin    7905 non-null   float64\n"," 17  Stage          7905 non-null   float64\n"," 18  Status         7905 non-null   object \n","dtypes: float64(10), int64(2), object(7)\n","memory usage: 1.1+ MB\n"]}],"source":["# Use info to see the number of categorical/text columns (any column with Dtype=object is text and will need to be converted to a number since ML models work only with numbers) \n","# Here you can also see if there are any missing values by looking at the number of non-null values in each column\n","train_df.info()\n","\n","# Make a list of categorical columns for future use (in the feature scaling section)\n","categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']"]},{"cell_type":"code","execution_count":3,"id":"778ad116","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:52.702784Z","iopub.status.busy":"2023-12-31T01:35:52.702407Z","iopub.status.idle":"2023-12-31T01:35:52.75816Z","shell.execute_reply":"2023-12-31T01:35:52.756828Z"},"papermill":{"duration":0.071499,"end_time":"2023-12-31T01:35:52.760745","exception":false,"start_time":"2023-12-31T01:35:52.689246","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2030.173308</td>\n","      <td>18373.146490</td>\n","      <td>2.594485</td>\n","      <td>350.561923</td>\n","      <td>3.548323</td>\n","      <td>83.902846</td>\n","      <td>1816.745250</td>\n","      <td>114.604602</td>\n","      <td>115.340164</td>\n","      <td>265.228969</td>\n","      <td>10.629462</td>\n","      <td>3.032511</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1094.233744</td>\n","      <td>3679.958739</td>\n","      <td>3.812960</td>\n","      <td>195.379344</td>\n","      <td>0.346171</td>\n","      <td>75.899266</td>\n","      <td>1903.750657</td>\n","      <td>48.790945</td>\n","      <td>52.530402</td>\n","      <td>87.465579</td>\n","      <td>0.781735</td>\n","      <td>0.866511</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>41.000000</td>\n","      <td>9598.000000</td>\n","      <td>0.300000</td>\n","      <td>120.000000</td>\n","      <td>1.960000</td>\n","      <td>4.000000</td>\n","      <td>289.000000</td>\n","      <td>26.350000</td>\n","      <td>33.000000</td>\n","      <td>62.000000</td>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1230.000000</td>\n","      <td>15574.000000</td>\n","      <td>0.700000</td>\n","      <td>248.000000</td>\n","      <td>3.350000</td>\n","      <td>39.000000</td>\n","      <td>834.000000</td>\n","      <td>75.950000</td>\n","      <td>84.000000</td>\n","      <td>211.000000</td>\n","      <td>10.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1831.000000</td>\n","      <td>18713.000000</td>\n","      <td>1.100000</td>\n","      <td>298.000000</td>\n","      <td>3.580000</td>\n","      <td>63.000000</td>\n","      <td>1181.000000</td>\n","      <td>108.500000</td>\n","      <td>104.000000</td>\n","      <td>265.000000</td>\n","      <td>10.600000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2689.000000</td>\n","      <td>20684.000000</td>\n","      <td>3.000000</td>\n","      <td>390.000000</td>\n","      <td>3.770000</td>\n","      <td>102.000000</td>\n","      <td>1857.000000</td>\n","      <td>137.950000</td>\n","      <td>139.000000</td>\n","      <td>316.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4795.000000</td>\n","      <td>28650.000000</td>\n","      <td>28.000000</td>\n","      <td>1775.000000</td>\n","      <td>4.640000</td>\n","      <td>588.000000</td>\n","      <td>13862.400000</td>\n","      <td>457.250000</td>\n","      <td>598.000000</td>\n","      <td>563.000000</td>\n","      <td>18.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days           Age    Bilirubin  Cholesterol      Albumin  \\\n","count  7905.000000   7905.000000  7905.000000  7905.000000  7905.000000   \n","mean   2030.173308  18373.146490     2.594485   350.561923     3.548323   \n","std    1094.233744   3679.958739     3.812960   195.379344     0.346171   \n","min      41.000000   9598.000000     0.300000   120.000000     1.960000   \n","25%    1230.000000  15574.000000     0.700000   248.000000     3.350000   \n","50%    1831.000000  18713.000000     1.100000   298.000000     3.580000   \n","75%    2689.000000  20684.000000     3.000000   390.000000     3.770000   \n","max    4795.000000  28650.000000    28.000000  1775.000000     4.640000   \n","\n","            Copper      Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  7905.000000   7905.000000  7905.000000    7905.000000  7905.000000   \n","mean     83.902846   1816.745250   114.604602     115.340164   265.228969   \n","std      75.899266   1903.750657    48.790945      52.530402    87.465579   \n","min       4.000000    289.000000    26.350000      33.000000    62.000000   \n","25%      39.000000    834.000000    75.950000      84.000000   211.000000   \n","50%      63.000000   1181.000000   108.500000     104.000000   265.000000   \n","75%     102.000000   1857.000000   137.950000     139.000000   316.000000   \n","max     588.000000  13862.400000   457.250000     598.000000   563.000000   \n","\n","       Prothrombin        Stage  \n","count  7905.000000  7905.000000  \n","mean     10.629462     3.032511  \n","std       0.781735     0.866511  \n","min       9.000000     1.000000  \n","25%      10.000000     2.000000  \n","50%      10.600000     3.000000  \n","75%      11.000000     4.000000  \n","max      18.000000     4.000000  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Use the describe method to generate statistical information such as standard deviation, mean, and min/max. \n","train_df.describe()"]},{"cell_type":"markdown","id":"996d39eb","metadata":{"papermill":{"duration":0.010721,"end_time":"2023-12-31T01:35:52.782265","exception":false,"start_time":"2023-12-31T01:35:52.771544","status":"completed"},"tags":[]},"source":["# Dataset Pre-Processing Task Overview\n","\n","1. Target Variable Label Encoding - Make sure the training labels are numerical, otherwise we cannot train the model. \n","2. Missing Feature Value Handling - Make sure the dataset is not missing any values, if so we cannot train the model.\n","3. Feature Scaling - Make sure the features are similar in numerical value, otherwise some ML models will struggle to weigh them appropriately during training.  \n","4. Categorical Attribute Handling (Should be done at the end since it may change the number of columns) - Make sure text values have been converted to numbers, otherwise ML algorithms cannot learn from text data.\n","5. Train/Test Split - Need to create a validation set for hyperparameter tuning and evaluation, otherwise the model will overfit the hyperparameters to the training dataset and your model will not generalize well to the real-world / Kaggle private test set. "]},{"cell_type":"markdown","id":"700cceed","metadata":{"papermill":{"duration":0.01283,"end_time":"2023-12-31T01:35:52.805739","exception":false,"start_time":"2023-12-31T01:35:52.792909","status":"completed"},"tags":[]},"source":["## Target Variable Label Encoding\n","\n","Problem: Since the Label column that we are trying to predict, 'Status', is a text column, we need to convert the column's values into integers so the machine learning model can process it (ML models do not support text input so we must convert the text into a numerical representation). We have three possible categories: Status_C, Status_CL, and Status_D. \n","\n","Possible Approaches:\n","1. Label Encoding - Convert each text category into an integer label. (Ex: 0, 1, 2)\n","2. Ordinal Encoding - Convert each text category into an integer label but with a particular order. Used when the categories have some quantitative order that can be taken advantage of like low, medium, high.   (Ex: low -> 0, medium-> 1, high -> 2)\n","3. One-Hot Encoding - Convert each text category into a separate column. For example, this is done in the softmax layer of a neural network. \n","\n","Chosen Approach: Label Encoding - There is no obvious ordering in the 'Status' column and ML libraries typically expect a single Label column which rules out one-hot encoding. Therefore we will use label encoding."]},{"cell_type":"code","execution_count":4,"id":"29e9c8e4","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:52.82959Z","iopub.status.busy":"2023-12-31T01:35:52.82907Z","iopub.status.idle":"2023-12-31T01:35:52.841609Z","shell.execute_reply":"2023-12-31T01:35:52.839953Z"},"papermill":{"duration":0.026997,"end_time":"2023-12-31T01:35:52.843988","exception":false,"start_time":"2023-12-31T01:35:52.816991","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","C     4965\n","D     2665\n","CL     275\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Check to see what the initial text categories are\n","train_df['Status'].value_counts()"]},{"cell_type":"code","execution_count":5,"id":"6ed13651","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:52.868944Z","iopub.status.busy":"2023-12-31T01:35:52.868512Z","iopub.status.idle":"2023-12-31T01:35:54.269115Z","shell.execute_reply":"2023-12-31T01:35:54.267947Z"},"papermill":{"duration":1.415252,"end_time":"2023-12-31T01:35:54.271662","exception":false,"start_time":"2023-12-31T01:35:52.85641","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","0    4965\n","2    2665\n","1     275\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Use sklearn's LabelEncoder class to transform the Status column's values from strings to integers.\n","label_encoder = LabelEncoder() \n","train_df['Status'] = label_encoder.fit_transform(train_df['Status'])\n","\n","# Check that the label encoder transformation was applied correctly and the categories are now numbers\n","train_df['Status'].value_counts()"]},{"cell_type":"markdown","id":"73c5a6cd","metadata":{"papermill":{"duration":0.010867,"end_time":"2023-12-31T01:35:54.294059","exception":false,"start_time":"2023-12-31T01:35:54.283192","status":"completed"},"tags":[]},"source":["## Missing Feature Value Handling\n","\n","Problem: We need to check if there are any missing feature values since most machine learning algorithms cannot handle an empty cell with no value. For example, a Logistic Regression model would throw an error in training although some ML algorithms like XGBoost are implemented to train normally by substituting in a value as needed. \n","\n","Solution: We check for missing feature values by using the isna DataFrame method which returns a boolean DataFrame where each cell is True if the value is missing and False otherwise. We then apply the sum method to find the number of missing values in each column. We find there are no missing values in this dataset so we can skip this step. "]},{"cell_type":"code","execution_count":6,"id":"e65434f8","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.319307Z","iopub.status.busy":"2023-12-31T01:35:54.318655Z","iopub.status.idle":"2023-12-31T01:35:54.332778Z","shell.execute_reply":"2023-12-31T01:35:54.331605Z"},"papermill":{"duration":0.030485,"end_time":"2023-12-31T01:35:54.335658","exception":false,"start_time":"2023-12-31T01:35:54.305173","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column: \n","N_Days           0\n","Drug             0\n","Age              0\n","Sex              0\n","Ascites          0\n","Hepatomegaly     0\n","Spiders          0\n","Edema            0\n","Bilirubin        0\n","Cholesterol      0\n","Albumin          0\n","Copper           0\n","Alk_Phos         0\n","SGOT             0\n","Tryglicerides    0\n","Platelets        0\n","Prothrombin      0\n","Stage            0\n","Status           0\n","dtype: int64\n"]}],"source":["# Check for missing feature values\n","print(\"Number of missing feature values by column: \")\n","print(train_df.isna().sum())"]},{"cell_type":"code","execution_count":7,"id":"e6a76790","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.360047Z","iopub.status.busy":"2023-12-31T01:35:54.359617Z","iopub.status.idle":"2023-12-31T01:35:54.371146Z","shell.execute_reply":"2023-12-31T01:35:54.369932Z"},"papermill":{"duration":0.026391,"end_time":"2023-12-31T01:35:54.373366","exception":false,"start_time":"2023-12-31T01:35:54.346975","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column in test data: 0\n"]}],"source":["# Alternatively we can check the total number of missing feature values in the dataset this way. \n","print(\"Number of missing feature values by column in test data:\", test_df.isna().sum().sum())"]},{"cell_type":"markdown","id":"e8ad4123","metadata":{"papermill":{"duration":0.0112,"end_time":"2023-12-31T01:35:54.396075","exception":false,"start_time":"2023-12-31T01:35:54.384875","status":"completed"},"tags":[]},"source":["## Feature Scaling\n","\n","Problem: Typically since feature column values are combined (often by adding them together) to create the final classification, the ML model will perform better if the features are on the same scale. (Ex: The ML model would struggle to scale values correctly if one column's values was in the billions and another column had values from 1-10)\n","\n","Possible Feature Scaling Approaches: \n","1. Min-Max Scaling (Default)- Scales the data to a fixed range between two values (typically 0 and 1). Most useful for neural networks.\n","2. Standardization (Default) - Scales the data so that the mean is 0 and the standard deviation is 1. Most useful for algorithms that assume a normal distribution of data, such as SVMs and logistic regression.\n","3. Robust Scaling (Advanced) - Scaling based on median and IQR. Most useful for handling significant outliers.\n","4. MaxAbsScaler (Advanced) - Scales each feature based on its maximum absolute value. Useful for sparse data. \n","\n","Chosen Approach: Standardization - Since XGBoost's decision tree classification uses splitting which occurs within a column, different column values do not interact with each other and therefore scaling the features is not necessary. However, since we are using Logistic Regression as our baseline model and features do interact in training we will need to do feature scaling. Since we are doing feature scaling specifically for our logistic regression model we will use the Standardization approach which is a commonly used approach that is effective for many datasets. "]},{"cell_type":"code","execution_count":8,"id":"c8cad08c","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.422054Z","iopub.status.busy":"2023-12-31T01:35:54.420783Z","iopub.status.idle":"2023-12-31T01:35:54.490564Z","shell.execute_reply":"2023-12-31T01:35:54.489284Z"},"papermill":{"duration":0.085497,"end_time":"2023-12-31T01:35:54.493114","exception":false,"start_time":"2023-12-31T01:35:54.407617","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.007790</td>\n","      <td>0.033864</td>\n","      <td>0.001549</td>\n","      <td>0.009851</td>\n","      <td>-0.029617</td>\n","      <td>0.010526</td>\n","      <td>-0.002895</td>\n","      <td>-0.020847</td>\n","      <td>-0.001029</td>\n","      <td>-0.013781</td>\n","      <td>0.004353</td>\n","      <td>0.005175</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.993309</td>\n","      <td>0.973958</td>\n","      <td>1.010406</td>\n","      <td>1.025961</td>\n","      <td>1.025240</td>\n","      <td>1.021709</td>\n","      <td>1.016664</td>\n","      <td>1.003627</td>\n","      <td>1.001441</td>\n","      <td>1.001418</td>\n","      <td>1.014105</td>\n","      <td>0.987967</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-1.817984</td>\n","      <td>-2.384728</td>\n","      <td>-0.601797</td>\n","      <td>-1.180148</td>\n","      <td>-4.588553</td>\n","      <td>-1.052815</td>\n","      <td>-0.802543</td>\n","      <td>-1.808946</td>\n","      <td>-1.567576</td>\n","      <td>-2.323678</td>\n","      <td>-2.084550</td>\n","      <td>-2.345776</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-0.727654</td>\n","      <td>-0.718300</td>\n","      <td>-0.496885</td>\n","      <td>-0.524971</td>\n","      <td>-0.572940</td>\n","      <td>-0.591648</td>\n","      <td>-0.522026</td>\n","      <td>-0.811772</td>\n","      <td>-0.596648</td>\n","      <td>-0.642910</td>\n","      <td>-0.805263</td>\n","      <td>-1.191649</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>-0.135421</td>\n","      <td>0.117632</td>\n","      <td>-0.391973</td>\n","      <td>-0.263923</td>\n","      <td>0.062625</td>\n","      <td>-0.249068</td>\n","      <td>-0.354452</td>\n","      <td>-0.156896</td>\n","      <td>-0.215892</td>\n","      <td>-0.071221</td>\n","      <td>-0.037691</td>\n","      <td>-0.037522</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.604869</td>\n","      <td>0.627996</td>\n","      <td>0.106359</td>\n","      <td>0.201867</td>\n","      <td>0.640411</td>\n","      <td>0.238452</td>\n","      <td>0.011428</td>\n","      <td>0.478508</td>\n","      <td>0.431393</td>\n","      <td>0.591939</td>\n","      <td>0.474024</td>\n","      <td>1.116605</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2.526884</td>\n","      <td>2.792831</td>\n","      <td>6.663359</td>\n","      <td>7.291089</td>\n","      <td>3.153780</td>\n","      <td>6.642081</td>\n","      <td>6.327728</td>\n","      <td>7.023169</td>\n","      <td>9.188781</td>\n","      <td>3.404652</td>\n","      <td>5.847030</td>\n","      <td>1.116605</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days          Age    Bilirubin  Cholesterol      Albumin  \\\n","count  5271.000000  5271.000000  5271.000000  5271.000000  5271.000000   \n","mean      0.007790     0.033864     0.001549     0.009851    -0.029617   \n","std       0.993309     0.973958     1.010406     1.025961     1.025240   \n","min      -1.817984    -2.384728    -0.601797    -1.180148    -4.588553   \n","25%      -0.727654    -0.718300    -0.496885    -0.524971    -0.572940   \n","50%      -0.135421     0.117632    -0.391973    -0.263923     0.062625   \n","75%       0.604869     0.627996     0.106359     0.201867     0.640411   \n","max       2.526884     2.792831     6.663359     7.291089     3.153780   \n","\n","            Copper     Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  5271.000000  5271.000000  5271.000000    5271.000000  5271.000000   \n","mean      0.010526    -0.002895    -0.020847      -0.001029    -0.013781   \n","std       1.021709     1.016664     1.003627       1.001441     1.001418   \n","min      -1.052815    -0.802543    -1.808946      -1.567576    -2.323678   \n","25%      -0.591648    -0.522026    -0.811772      -0.596648    -0.642910   \n","50%      -0.249068    -0.354452    -0.156896      -0.215892    -0.071221   \n","75%       0.238452     0.011428     0.478508       0.431393     0.591939   \n","max       6.642081     6.327728     7.023169       9.188781     3.404652   \n","\n","       Prothrombin        Stage  \n","count  5271.000000  5271.000000  \n","mean      0.004353     0.005175  \n","std       1.014105     0.987967  \n","min      -2.084550    -2.345776  \n","25%      -0.805263    -1.191649  \n","50%      -0.037691    -0.037522  \n","75%       0.474024     1.116605  \n","max       5.847030     1.116605  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler \n","\n","# Here we find the numerical columns that need to be scaled by removing the text columns from the list of total columns in the df.\n","numerical_cols = train_df.columns.difference(categorical_cols)\n","\n","# Now we use the StandardScaler class from sklearn to transform our numerical columns to the a Standardized scale\n","std_scaler = StandardScaler() \n","std_scaler.fit(train_df[numerical_cols])\n","\n","train_df[numerical_cols] = std_scaler.transform(train_df[numerical_cols])\n","test_df[numerical_cols] = std_scaler.transform(test_df[numerical_cols])\n","                      \n","# Confirm the transformation was successful by seeing if the mean = 0 and std = 1 for numerical columns\n","test_df.describe()"]},{"cell_type":"markdown","id":"f9348bc0","metadata":{"papermill":{"duration":0.011153,"end_time":"2023-12-31T01:35:54.515796","exception":false,"start_time":"2023-12-31T01:35:54.504643","status":"completed"},"tags":[]},"source":["## Handling Categorical Attributes/Columns \n","Problem: ML models cannot handle text data naturally, they can only handle numbers so we need to convert text data into some numerical representation that still contains the relevant information. \n","\n","Possible Approaches: The main approaches for categorical attribute handling are \n","1. Ordinal Encoding - Useful when the categories correspond to an ascending or descending order. \n","2. One-Hot Encoding (Default Choice) - For each categorical column, convert it into multiple columns, one for each possible category. This is used when the categories do not have an obvious logical order. \n","3. Numerical Feature Replacement (Advanced) - In cases where the number of categories is cery large (hundreds or thousands) one should consider replacing the categorical columns with a numerical column that converts each category into some number. For example, one could convert a country code into the country's population. \n","4. Embedding Replacement (Advanced) - Alternatively, one can replace categories with embeddings, which are low dimensional vectors that represent the category. \n","\n","Chosen Approach : One-Hot Encoding - In this case, we use a one-hot encoding since none of the categories seem to have an order to the classes (ruling out ordinal encoding) and the number of categories for each column is low (under 10 for all categorical columns) which rules out needing advanced methods. "]},{"cell_type":"code","execution_count":9,"id":"43effa8f","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.541778Z","iopub.status.busy":"2023-12-31T01:35:54.5414Z","iopub.status.idle":"2023-12-31T01:35:54.558298Z","shell.execute_reply":"2023-12-31T01:35:54.556885Z"},"papermill":{"duration":0.03361,"end_time":"2023-12-31T01:35:54.561167","exception":false,"start_time":"2023-12-31T01:35:54.527557","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["N_Days           461\n","Drug               2\n","Age              391\n","Sex                2\n","Ascites            2\n","Hepatomegaly       2\n","Spiders            2\n","Edema              3\n","Bilirubin        111\n","Cholesterol      226\n","Albumin          160\n","Copper           171\n","Alk_Phos         364\n","SGOT             206\n","Tryglicerides    154\n","Platelets        227\n","Prothrombin       49\n","Stage              4\n","Status             3\n","dtype: int64\n"]}],"source":["# Confirm that the number of categories in the categorical columns is manageable (< 100) since we will be adding a column to the df for each category\n","unique_values_per_column = train_df.nunique()\n","print(unique_values_per_column)"]},{"cell_type":"code","execution_count":10,"id":"0268b745","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.589163Z","iopub.status.busy":"2023-12-31T01:35:54.588508Z","iopub.status.idle":"2023-12-31T01:35:54.633972Z","shell.execute_reply":"2023-12-31T01:35:54.632301Z"},"papermill":{"duration":0.062889,"end_time":"2023-12-31T01:35:54.636841","exception":false,"start_time":"2023-12-31T01:35:54.573952","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5271 entries, 0 to 5270\n","Data columns (total 25 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   N_Days                5271 non-null   float64\n"," 1   Age                   5271 non-null   float64\n"," 2   Bilirubin             5271 non-null   float64\n"," 3   Cholesterol           5271 non-null   float64\n"," 4   Albumin               5271 non-null   float64\n"," 5   Copper                5271 non-null   float64\n"," 6   Alk_Phos              5271 non-null   float64\n"," 7   SGOT                  5271 non-null   float64\n"," 8   Tryglicerides         5271 non-null   float64\n"," 9   Platelets             5271 non-null   float64\n"," 10  Prothrombin           5271 non-null   float64\n"," 11  Stage                 5271 non-null   float64\n"," 12  Drug_D-penicillamine  5271 non-null   bool   \n"," 13  Drug_Placebo          5271 non-null   bool   \n"," 14  Sex_F                 5271 non-null   bool   \n"," 15  Sex_M                 5271 non-null   bool   \n"," 16  Ascites_N             5271 non-null   bool   \n"," 17  Ascites_Y             5271 non-null   bool   \n"," 18  Hepatomegaly_N        5271 non-null   bool   \n"," 19  Hepatomegaly_Y        5271 non-null   bool   \n"," 20  Spiders_N             5271 non-null   bool   \n"," 21  Spiders_Y             5271 non-null   bool   \n"," 22  Edema_N               5271 non-null   bool   \n"," 23  Edema_S               5271 non-null   bool   \n"," 24  Edema_Y               5271 non-null   bool   \n","dtypes: bool(13), float64(12)\n","memory usage: 561.2 KB\n"]}],"source":["# Convert the categorical columns into one-hot encodings using the get_dummies function\n","status = train_df['Status']\n","train_df_dummies = pd.get_dummies(train_df.drop('Status', axis=1))\n","train_df = pd.concat([train_df_dummies, status], axis=1)\n","test_df = pd.get_dummies(test_df)\n","\n","# Confirm the transformation was successful\n","test_df.info()"]},{"cell_type":"markdown","id":"ab319278","metadata":{"papermill":{"duration":0.011843,"end_time":"2023-12-31T01:35:54.661429","exception":false,"start_time":"2023-12-31T01:35:54.649586","status":"completed"},"tags":[]},"source":["## Train/Test Split\n","\n","Problem: We need to split the dataset into a training dataset and a testing/tuning dataset. If we were not to do this and did both training / tuning / evaluation with the same dataset, the final model would likely be overfitted and would not generalize well to the real-world / the Kaggle competition private test set. \n","\n","Possible Approaches\n","1. K-Fold Cross-Validation - In this approach the dataset is divided into k equal-sized subsets. Then we train the model k times, each time using a different subset as the validation set and the other k-1 subsets as the training set. Finally we use the average score among all k models as the final score. This approach is commonly used since it essentially utilizes  more of the training data for validation (90% vs. 80%). Typical values range are k=5 or k=10. \n","2. Train/Validation/Test Split - In this approach we split the dataset into three distinct sets: a training set for training the model, a validation set for tuning the hyperparameters and a test set for evaluating the final model. By separating the test set and validation set we reduce / avoid overfitting to the test set. A typical split varies but could be 80/10/10 or 70/15/15.\n","\n","Chosen Approach : Initially we used the train/validation/test approach since it is simpler to implement for manual hyperparameter tuning. However later in some cases we used k-fold cross-validation since that is the default for automated hyperparameter tuning methods such as GridSearchCV.  "]},{"cell_type":"code","execution_count":11,"id":"52e6219a","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.688377Z","iopub.status.busy":"2023-12-31T01:35:54.687958Z","iopub.status.idle":"2023-12-31T01:35:54.822031Z","shell.execute_reply":"2023-12-31T01:35:54.820525Z"},"papermill":{"duration":0.15101,"end_time":"2023-12-31T01:35:54.825183","exception":false,"start_time":"2023-12-31T01:35:54.674173","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Set parameters for k-folds cross-validation\n","kfold = StratifiedKFold(n_splits=10, shuffle=True)\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split the training set into a training and validation set \n","X = train_df.drop(\"Status\", axis=1)\n","y = train_df[\"Status\"]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y,  test_size = 0.1)\n"]},{"cell_type":"markdown","id":"8ea1e187","metadata":{"papermill":{"duration":0.013239,"end_time":"2023-12-31T01:35:54.85078","exception":false,"start_time":"2023-12-31T01:35:54.837541","status":"completed"},"tags":[]},"source":["# Logistic Regression Baseline Model\n","Default Model Performance: 0.506\n","\n","Tuned Model Performance: 0.506\n","\n","Hyperparameter Tuning: \n","Here we manually tuned the most important parameters for Logistic Regression (C, max_iter, class_weight, tol, penalty, solver) but were unable to significantly increase performance (still around 0.506)\n","\n","Conclusion: We set the baseline of model performance at 0.506 log-loss since Logistic Regression is the simplest model we used on the dataset. Since the performance was so weak, we decided to not include Logistic Regression in the final ensemble of models for competition submission.  "]},{"cell_type":"code","execution_count":12,"id":"41858e17","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:54.876184Z","iopub.status.busy":"2023-12-31T01:35:54.875755Z","iopub.status.idle":"2023-12-31T01:35:55.571744Z","shell.execute_reply":"2023-12-31T01:35:55.570599Z"},"papermill":{"duration":0.712991,"end_time":"2023-12-31T01:35:55.575643","exception":false,"start_time":"2023-12-31T01:35:54.862652","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Set Log Loss: 0.5687626719650675\n"]}],"source":["from sklearn.linear_model import LogisticRegression \n","from sklearn.metrics import log_loss\n","\n","# Train the model using sci-kit learn's Logistic Regression model \n","model = LogisticRegression(C=1, max_iter = 1000)\n","# The fit method learns the parameters (weights) for the model\n","model.fit(X_train, y_train)\n","# Predict the probability of each class for the validation set and the test set \n","y_val_pred_lr = model.predict_proba(X_val)\n","y_pred_lr = model.predict_proba(test_df)\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_lr)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"7b1ab831","metadata":{"papermill":{"duration":0.024561,"end_time":"2023-12-31T01:35:55.625854","exception":false,"start_time":"2023-12-31T01:35:55.601293","status":"completed"},"tags":[]},"source":["# Random Forest Model \n","Default Model Performance: 0.438\n","\n","Tuned Model Performance: 0.434 \n","\n","Hyperparameter Tuning:\n","We manually tuned hyperparameters starting from the most important hyperparameters (n_estimators) and working through the rest (max_depth, max_features, min_samples_split, min_impurity_decrease, min_samples_leaf, min_weight_fraction_leaf). We only found significant improvements in performance by tuning the n_estimator hyperparameter which improved performance from 0.438 -> 0.434.\n","\n","Conclusion: The overall conclusion is that the Random Forest model performed significantly better than the baseline Logistic Regression model with a log-loss of 0.434 compared to 0.506. However, this model still performed significantly worse than the best XGBoost and LGBM models so we decided to exclude the Random Forest from the final ensemble for competition submission. \n"]},{"cell_type":"code","execution_count":13,"id":"018a9cb1","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:35:55.680809Z","iopub.status.busy":"2023-12-31T01:35:55.679376Z","iopub.status.idle":"2023-12-31T01:36:01.340076Z","shell.execute_reply":"2023-12-31T01:36:01.338679Z"},"papermill":{"duration":5.691318,"end_time":"2023-12-31T01:36:01.343391","exception":false,"start_time":"2023-12-31T01:35:55.652073","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.95333333 0.02333333 0.02333333]\n"," [0.43333333 0.11333333 0.45333333]\n"," [0.94666667 0.01333333 0.04      ]\n"," ...\n"," [0.99       0.         0.01      ]\n"," [0.86666667 0.         0.13333333]\n"," [0.39333333 0.12333333 0.48333333]] \n","\n","Validation Set Log Loss: 0.48580355076022097\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Train the model using sci-kit learn's Random Forest model \n","rf_classifier = RandomForestClassifier(n_estimators=300)\n","# The fit method learns the parameters (weights) for the model\n","rf_classifier.fit(X_train, y_train) \n","# Predict the probability of each class for the validation set and the test set (Kaggle's unlabeled dataset)\n","y_val_pred_rf = rf_classifier.predict_proba(X_val)\n","y_pred_rf = rf_classifier.predict_proba(test_df)\n","print(y_val_pred_rf, \"\\n\")\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_rf)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"0213c4b1","metadata":{"papermill":{"duration":0.012285,"end_time":"2023-12-31T01:36:01.368214","exception":false,"start_time":"2023-12-31T01:36:01.355929","status":"completed"},"tags":[]},"source":["# XGBoost Model\n","Default Model Performance: 0.457\n","\n","Tuned Model Performance: 0.38\n","\n","Approach: Initially we tuned the model hyperparameters manually as before which is an approach I've seen suggested in some ML books such as Corey Wade's XGBoost book. However, we were not able to tune the model beyond ~0.42 score. \n","\n","Conclusion: After comparing to other XGboost notebooks it was clear in order to tune the model hyperparameters manually one needs a great deal of experience. For example, in the method I have been using where you tune hyperparameters one at a time by simply changing the numbers in the XGBClassifier(n_estimators=700) constructor call, n_estimators=700 performs very poorly and therefore is discarded early in the process. However, it turns out n_estimators=700 performs very well *if* the learning_rate is also tuned at the same time. This suggests I need to improve my understanding of how hyperparameters interact and automate some of my process using GridSearchCV for future hyperparameter tuning. \n"]},{"cell_type":"code","execution_count":14,"id":"1b838f5d","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:36:01.395601Z","iopub.status.busy":"2023-12-31T01:36:01.394785Z","iopub.status.idle":"2023-12-31T01:36:04.780748Z","shell.execute_reply":"2023-12-31T01:36:04.779841Z"},"papermill":{"duration":3.402427,"end_time":"2023-12-31T01:36:04.783255","exception":false,"start_time":"2023-12-31T01:36:01.380828","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[9.6170044e-01 2.0416584e-02 1.7883029e-02]\n"," [6.0840321e-01 3.6197193e-02 3.5539961e-01]\n"," [8.9683765e-01 4.8520744e-02 5.4641582e-02]\n"," ...\n"," [9.4769120e-01 3.8396190e-03 4.8469141e-02]\n"," [9.7596639e-01 8.7538926e-04 2.3158232e-02]\n"," [4.0969661e-01 2.1907842e-01 3.7122494e-01]] \n","\n","Validation Set Log Loss: 0.44845323144121374\n"]}],"source":["from xgboost import XGBClassifier \n","\n","# Train the model using xgboost's XGBClassifier model \n","xgb_model = XGBClassifier(n_estimators=700, max_depth=6, learning_rate=0.04,colsample_bytree=0.168, min_child_weight=17, subsample=0.7) \n","# The fit method learns the parameters (weights) for the model\n","xgb_model.fit(X_train, y_train)\n","# Predict the probability of each class for the validation set and the test set (Kaggle's unlabeled dataset)\n","y_val_pred_xgb = xgb_model.predict_proba(X_val)\n","y_pred_xgb = xgb_model.predict_proba(test_df)\n","print(y_val_pred_xgb, \"\\n\")\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_xgb)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"fd288df5","metadata":{"papermill":{"duration":0.012606,"end_time":"2023-12-31T01:36:04.809482","exception":false,"start_time":"2023-12-31T01:36:04.796876","status":"completed"},"tags":[]},"source":["# LGBM Classifier\n","Default Model Performance: 0.455\n","\n","Tuned Model Performance: 0.396\n","\n","Approach  / Conclusion: The approach and conclusions are similar to those found in the XGBoost section. "]},{"cell_type":"code","execution_count":15,"id":"4711c90e","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:36:04.836304Z","iopub.status.busy":"2023-12-31T01:36:04.835347Z","iopub.status.idle":"2023-12-31T01:36:12.983856Z","shell.execute_reply":"2023-12-31T01:36:12.982917Z"},"papermill":{"duration":8.164619,"end_time":"2023-12-31T01:36:12.986355","exception":false,"start_time":"2023-12-31T01:36:04.821736","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Set Log Loss: 0.4474931502962795\n"]}],"source":["import lightgbm as lgb\n","\n","# Define your parameters\n","lgb_params = {\n","    'max_depth': 15,\n","    'min_child_samples': 13,\n","    'learning_rate': 0.05285597081335651,\n","    'n_estimators': 294,\n","    'min_child_weight': 5,\n","    'colsample_bytree': 0.10012816493265511,\n","    'reg_alpha': 0.8767668608061822,\n","    'reg_lambda': 0.8705834466355764\n","}\n","\n","# Create the LGBMClassifier with the specified parameters\n","lgbm_model = lgb.LGBMClassifier(**lgb_params)\n","\n","# Now you can fit this classifier to your data\n","lgbm_model.fit(X_train, y_train)\n","\n","# And make predictions\n","y_val_pred_lgbm = lgbm_model.predict_proba(X_val)\n","y_pred_lgbm = lgbm_model.predict_proba(test_df)\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_lgbm)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"ba3ea48e","metadata":{"papermill":{"duration":0.012062,"end_time":"2023-12-31T01:36:13.010927","exception":false,"start_time":"2023-12-31T01:36:12.998865","status":"completed"},"tags":[]},"source":["# Ensemble Model\n","\n","Approach: Since our current Logistic Regression and Random Forest models are significantly weaker than the XGBoost and LGBM models, we will only use the tuned XGBoost / LGBM models in the Ensemble. Further work will involve trying to tune the other models further so they can be included in the Ensemble. \n","\n","Tuned Ensemble Model Score: 0.398"]},{"cell_type":"code","execution_count":16,"id":"fd31846a","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:36:13.037883Z","iopub.status.busy":"2023-12-31T01:36:13.037158Z","iopub.status.idle":"2023-12-31T01:36:25.192779Z","shell.execute_reply":"2023-12-31T01:36:25.191464Z"},"papermill":{"duration":12.172255,"end_time":"2023-12-31T01:36:25.195459","exception":false,"start_time":"2023-12-31T01:36:13.023204","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Set Log Loss: 0.4457463408842565\n"]}],"source":["from sklearn.ensemble import VotingClassifier\n","\n","# Train an Ensemble model using a combination of the XGBoost and LGBM Classifiers, voting='soft' is used since we are predicting probabilities, not the actual classes\n","ensemble_model = VotingClassifier(\n","    estimators=[\n","        ('lgb', lgbm_model),\n","        ('xgb', xgb_model),\n","    ],\n","    voting='soft'\n",")\n","\n","ensemble_model.fit(X_train, y_train)\n","\n","# Make predictions using the ensemble\n","y_val_pred_ensemble = ensemble_model.predict_proba(X_val)\n","y_pred_ensemble = ensemble_model.predict_proba(test_df)\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_ensemble)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"code","execution_count":17,"id":"93d4baa5","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:36:25.222976Z","iopub.status.busy":"2023-12-31T01:36:25.222567Z","iopub.status.idle":"2023-12-31T01:36:38.511721Z","shell.execute_reply":"2023-12-31T01:36:38.510385Z"},"papermill":{"duration":13.306072,"end_time":"2023-12-31T01:36:38.514456","exception":false,"start_time":"2023-12-31T01:36:25.208384","status":"completed"},"tags":[]},"outputs":[],"source":["# Train the final Ensemble model for competition submission using all the data, including data from the validation set \n","ensemble_model_final = VotingClassifier(\n","    estimators=[\n","        ('lgb', lgbm_model),\n","        ('xgb', xgb_model),\n","    ],\n","    voting='soft'\n",")\n","\n","ensemble_model_final.fit(X, y)\n","\n","# Make predictions using the ensemble\n","y_pred_ensemble_final = ensemble_model.predict_proba(test_df)"]},{"cell_type":"markdown","id":"23ef09b9","metadata":{"papermill":{"duration":0.07652,"end_time":"2023-12-31T01:36:38.603342","exception":false,"start_time":"2023-12-31T01:36:38.526822","status":"completed"},"tags":[]},"source":["# Kaggle Submission Processing\n","1. Create a submission dataframe from the model's predictions \n","2. Concatenate the data id column values to adhere to submission formatting requirements\n","3. Convert the submission dataframe into a csv file for submission \n","4. Now in order to submit to Kaggle, save the notebook and navigate to the Submissions page for this competition and click 'Submit Prediction' in the top-right corner -> Notebook -> Submit. "]},{"cell_type":"code","execution_count":18,"id":"9da36ac2","metadata":{"execution":{"iopub.execute_input":"2023-12-31T01:36:38.630308Z","iopub.status.busy":"2023-12-31T01:36:38.629417Z","iopub.status.idle":"2023-12-31T01:36:38.685205Z","shell.execute_reply":"2023-12-31T01:36:38.684077Z"},"papermill":{"duration":0.072177,"end_time":"2023-12-31T01:36:38.687829","exception":false,"start_time":"2023-12-31T01:36:38.615652","status":"completed"},"tags":[]},"outputs":[],"source":["# Modify the probability predictions into the submission format \n","submission_df = pd.DataFrame(y_pred_ensemble_final, columns=['Status_C', 'Status_CL', 'Status_D'])\n","final_submission_df = pd.concat([test_id_df, submission_df], axis=1)\n","final_submission_df.head(10)\n","\n","# Create a submission.csv file that Kaggle will automatically evaluate for submission\n","final_submission_df.to_csv('submission.csv', index = False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7000181,"sourceId":60893,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":51.139383,"end_time":"2023-12-31T01:36:39.523081","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-31T01:35:48.383698","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}