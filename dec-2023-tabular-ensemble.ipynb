{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/jonathankao/dec-2023-tabular-xgboost?scriptVersionId=157045087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"b69c3c3b","metadata":{"papermill":{"duration":0.010441,"end_time":"2023-12-30T01:18:02.767529","exception":false,"start_time":"2023-12-30T01:18:02.757088","status":"completed"},"tags":[]},"source":["# Task Overview \n","Task: In this synthetic dataset based off of a real dataset funded by the Mayo Clinic, each example represents both general and survival information about a patient that has liver cirrhosis, a condition involving prolonged liver damage. The goal is to train a machine learning model that can predict the patient's current survival status based on the data features. \n","\n","Approach v1.0: Our approach will be to train a Logistic Regression model using sklearn to be used as a baseilne model and to then train a performance-focused model using XGBoost as a learning exercise.\n","\n","Approach v2.0: After a few days of initial work, we realized that it was feasible within the competition timeframe to improve on Approach 1 by training an Ensemble of models so we changed our approach to training multiple Logistic Regression, Random Forest, and XGBoost models and then combining them into an ensemble.\n","\n","Next Steps: Add hyperparameter tuning for each model and combine the models together to form an ensemble.  \n","\n","Version History\n","1. v1.0-1.3 - Implemented dataset loading and dataset pre-processing. Added explanations and comments for each step. \n","2. v1.4 - Added Logistic Regression model training and Kaggle submission formatting\n","3. v2.0 - Added Random Forest and XGBoost model training and prediction.\n","4. v2.1 - Added log-loss calculations for each model on the validation set. \n","5. v2.2 - Added initial hyperparameter tuning for XGBoost\n","6. v2.3 - Added LGBM Classifier. Added model consisting of an ensemble of tuned Random Forest, XGBoost, and LGBM models. \n","\n","Results Log\n","1. Initial Logistic Regression Model - ~.52 log-loss score\n","2. Initial Random Forest Model - ~.45 log-loss score\n","3. Initial XGBoost Model - ~.42 log-loss score\n","4. Tuned XGBoost Model ~.404 log-loss score \n","5. Tuned LGBM Model - ~.411 log-loss score\n","6. Ensemble Model - ~.445 log-loss score "]},{"cell_type":"markdown","id":"0a278ce3","metadata":{"papermill":{"duration":0.010776,"end_time":"2023-12-30T01:18:02.788735","exception":false,"start_time":"2023-12-30T01:18:02.777959","status":"completed"},"tags":[]},"source":["# Dataset Loading"]},{"cell_type":"code","execution_count":1,"id":"f1ffa010","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:02.811286Z","iopub.status.busy":"2023-12-30T01:18:02.810483Z","iopub.status.idle":"2023-12-30T01:18:03.921638Z","shell.execute_reply":"2023-12-30T01:18:03.919944Z"},"papermill":{"duration":1.125776,"end_time":"2023-12-30T01:18:03.924433","exception":false,"start_time":"2023-12-30T01:18:02.798657","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Drug</th>\n","      <th>Age</th>\n","      <th>Sex</th>\n","      <th>Ascites</th>\n","      <th>Hepatomegaly</th>\n","      <th>Spiders</th>\n","      <th>Edema</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","      <th>Status</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>999</td>\n","      <td>D-penicillamine</td>\n","      <td>21532</td>\n","      <td>M</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>2.3</td>\n","      <td>316.0</td>\n","      <td>3.35</td>\n","      <td>172.0</td>\n","      <td>1601.0</td>\n","      <td>179.80</td>\n","      <td>63.0</td>\n","      <td>394.0</td>\n","      <td>9.7</td>\n","      <td>3.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2574</td>\n","      <td>Placebo</td>\n","      <td>19237</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.9</td>\n","      <td>364.0</td>\n","      <td>3.54</td>\n","      <td>63.0</td>\n","      <td>1440.0</td>\n","      <td>134.85</td>\n","      <td>88.0</td>\n","      <td>361.0</td>\n","      <td>11.0</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3428</td>\n","      <td>Placebo</td>\n","      <td>13727</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>Y</td>\n","      <td>3.3</td>\n","      <td>299.0</td>\n","      <td>3.55</td>\n","      <td>131.0</td>\n","      <td>1029.0</td>\n","      <td>119.35</td>\n","      <td>50.0</td>\n","      <td>199.0</td>\n","      <td>11.7</td>\n","      <td>4.0</td>\n","      <td>D</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2576</td>\n","      <td>Placebo</td>\n","      <td>18460</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>0.6</td>\n","      <td>256.0</td>\n","      <td>3.50</td>\n","      <td>58.0</td>\n","      <td>1653.0</td>\n","      <td>71.30</td>\n","      <td>96.0</td>\n","      <td>269.0</td>\n","      <td>10.7</td>\n","      <td>3.0</td>\n","      <td>C</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>788</td>\n","      <td>Placebo</td>\n","      <td>16658</td>\n","      <td>F</td>\n","      <td>N</td>\n","      <td>Y</td>\n","      <td>N</td>\n","      <td>N</td>\n","      <td>1.1</td>\n","      <td>346.0</td>\n","      <td>3.65</td>\n","      <td>63.0</td>\n","      <td>1181.0</td>\n","      <td>125.55</td>\n","      <td>96.0</td>\n","      <td>298.0</td>\n","      <td>10.6</td>\n","      <td>4.0</td>\n","      <td>C</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n","0     999  D-penicillamine  21532   M       N            N       N     N   \n","1    2574          Placebo  19237   F       N            N       N     N   \n","2    3428          Placebo  13727   F       N            Y       Y     Y   \n","3    2576          Placebo  18460   F       N            N       N     N   \n","4     788          Placebo  16658   F       N            Y       N     N   \n","\n","   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n","0        2.3        316.0     3.35   172.0    1601.0  179.80           63.0   \n","1        0.9        364.0     3.54    63.0    1440.0  134.85           88.0   \n","2        3.3        299.0     3.55   131.0    1029.0  119.35           50.0   \n","3        0.6        256.0     3.50    58.0    1653.0   71.30           96.0   \n","4        1.1        346.0     3.65    63.0    1181.0  125.55           96.0   \n","\n","   Platelets  Prothrombin  Stage Status  \n","0      394.0          9.7    3.0      D  \n","1      361.0         11.0    3.0      C  \n","2      199.0         11.7    4.0      D  \n","3      269.0         10.7    3.0      C  \n","4      298.0         10.6    4.0      C  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","import pandas as pd\n","\n","# Read in the training and test data as Pandas DataFrames \n","train_file_path = \"/kaggle/input/playground-series-s3e26/train.csv\"\n","train_df = pd.read_csv(train_file_path) \n","test_file_path = \"/kaggle/input/playground-series-s3e26/test.csv\"\n","test_df = pd.read_csv(test_file_path)\n","\n","# Remove the id column since it is not useful for prediction\n","test_id_df = test_df['id'].astype(int)\n","train_df = train_df.drop('id', axis=1)\n","test_df = test_df.drop('id', axis=1)\n","\n","# Use the head method to visually see that the dataset has been loaded\n","train_df.head(5)"]},{"cell_type":"code","execution_count":2,"id":"c85c3f7e","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:03.948942Z","iopub.status.busy":"2023-12-30T01:18:03.94852Z","iopub.status.idle":"2023-12-30T01:18:03.981884Z","shell.execute_reply":"2023-12-30T01:18:03.979926Z"},"papermill":{"duration":0.049096,"end_time":"2023-12-30T01:18:03.98489","exception":false,"start_time":"2023-12-30T01:18:03.935794","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 7905 entries, 0 to 7904\n","Data columns (total 19 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   N_Days         7905 non-null   int64  \n"," 1   Drug           7905 non-null   object \n"," 2   Age            7905 non-null   int64  \n"," 3   Sex            7905 non-null   object \n"," 4   Ascites        7905 non-null   object \n"," 5   Hepatomegaly   7905 non-null   object \n"," 6   Spiders        7905 non-null   object \n"," 7   Edema          7905 non-null   object \n"," 8   Bilirubin      7905 non-null   float64\n"," 9   Cholesterol    7905 non-null   float64\n"," 10  Albumin        7905 non-null   float64\n"," 11  Copper         7905 non-null   float64\n"," 12  Alk_Phos       7905 non-null   float64\n"," 13  SGOT           7905 non-null   float64\n"," 14  Tryglicerides  7905 non-null   float64\n"," 15  Platelets      7905 non-null   float64\n"," 16  Prothrombin    7905 non-null   float64\n"," 17  Stage          7905 non-null   float64\n"," 18  Status         7905 non-null   object \n","dtypes: float64(10), int64(2), object(7)\n","memory usage: 1.1+ MB\n"]}],"source":["# Use info to see the number of categorical columns (any column with Dtype=object is text and will need to be converted to a number) \n","# Here you can also see if there are any missing values by looking at the non-null values in each column\n","train_df.info()\n","\n","# Make a list of categorical columns for future use (in the feature scaling section)\n","categorical_cols = ['Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema', 'Status']"]},{"cell_type":"code","execution_count":3,"id":"c4ea3f8e","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:04.009858Z","iopub.status.busy":"2023-12-30T01:18:04.009394Z","iopub.status.idle":"2023-12-30T01:18:04.071291Z","shell.execute_reply":"2023-12-30T01:18:04.069942Z"},"papermill":{"duration":0.078597,"end_time":"2023-12-30T01:18:04.074999","exception":false,"start_time":"2023-12-30T01:18:03.996402","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","      <td>7905.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2030.173308</td>\n","      <td>18373.146490</td>\n","      <td>2.594485</td>\n","      <td>350.561923</td>\n","      <td>3.548323</td>\n","      <td>83.902846</td>\n","      <td>1816.745250</td>\n","      <td>114.604602</td>\n","      <td>115.340164</td>\n","      <td>265.228969</td>\n","      <td>10.629462</td>\n","      <td>3.032511</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1094.233744</td>\n","      <td>3679.958739</td>\n","      <td>3.812960</td>\n","      <td>195.379344</td>\n","      <td>0.346171</td>\n","      <td>75.899266</td>\n","      <td>1903.750657</td>\n","      <td>48.790945</td>\n","      <td>52.530402</td>\n","      <td>87.465579</td>\n","      <td>0.781735</td>\n","      <td>0.866511</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>41.000000</td>\n","      <td>9598.000000</td>\n","      <td>0.300000</td>\n","      <td>120.000000</td>\n","      <td>1.960000</td>\n","      <td>4.000000</td>\n","      <td>289.000000</td>\n","      <td>26.350000</td>\n","      <td>33.000000</td>\n","      <td>62.000000</td>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1230.000000</td>\n","      <td>15574.000000</td>\n","      <td>0.700000</td>\n","      <td>248.000000</td>\n","      <td>3.350000</td>\n","      <td>39.000000</td>\n","      <td>834.000000</td>\n","      <td>75.950000</td>\n","      <td>84.000000</td>\n","      <td>211.000000</td>\n","      <td>10.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>1831.000000</td>\n","      <td>18713.000000</td>\n","      <td>1.100000</td>\n","      <td>298.000000</td>\n","      <td>3.580000</td>\n","      <td>63.000000</td>\n","      <td>1181.000000</td>\n","      <td>108.500000</td>\n","      <td>104.000000</td>\n","      <td>265.000000</td>\n","      <td>10.600000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>2689.000000</td>\n","      <td>20684.000000</td>\n","      <td>3.000000</td>\n","      <td>390.000000</td>\n","      <td>3.770000</td>\n","      <td>102.000000</td>\n","      <td>1857.000000</td>\n","      <td>137.950000</td>\n","      <td>139.000000</td>\n","      <td>316.000000</td>\n","      <td>11.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4795.000000</td>\n","      <td>28650.000000</td>\n","      <td>28.000000</td>\n","      <td>1775.000000</td>\n","      <td>4.640000</td>\n","      <td>588.000000</td>\n","      <td>13862.400000</td>\n","      <td>457.250000</td>\n","      <td>598.000000</td>\n","      <td>563.000000</td>\n","      <td>18.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days           Age    Bilirubin  Cholesterol      Albumin  \\\n","count  7905.000000   7905.000000  7905.000000  7905.000000  7905.000000   \n","mean   2030.173308  18373.146490     2.594485   350.561923     3.548323   \n","std    1094.233744   3679.958739     3.812960   195.379344     0.346171   \n","min      41.000000   9598.000000     0.300000   120.000000     1.960000   \n","25%    1230.000000  15574.000000     0.700000   248.000000     3.350000   \n","50%    1831.000000  18713.000000     1.100000   298.000000     3.580000   \n","75%    2689.000000  20684.000000     3.000000   390.000000     3.770000   \n","max    4795.000000  28650.000000    28.000000  1775.000000     4.640000   \n","\n","            Copper      Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  7905.000000   7905.000000  7905.000000    7905.000000  7905.000000   \n","mean     83.902846   1816.745250   114.604602     115.340164   265.228969   \n","std      75.899266   1903.750657    48.790945      52.530402    87.465579   \n","min       4.000000    289.000000    26.350000      33.000000    62.000000   \n","25%      39.000000    834.000000    75.950000      84.000000   211.000000   \n","50%      63.000000   1181.000000   108.500000     104.000000   265.000000   \n","75%     102.000000   1857.000000   137.950000     139.000000   316.000000   \n","max     588.000000  13862.400000   457.250000     598.000000   563.000000   \n","\n","       Prothrombin        Stage  \n","count  7905.000000  7905.000000  \n","mean     10.629462     3.032511  \n","std       0.781735     0.866511  \n","min       9.000000     1.000000  \n","25%      10.000000     2.000000  \n","50%      10.600000     3.000000  \n","75%      11.000000     4.000000  \n","max      18.000000     4.000000  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Use the describe method to generate statistical information such as standard deviation, mean, and min/max. \n","train_df.describe()"]},{"cell_type":"markdown","id":"a24e77d5","metadata":{"papermill":{"duration":0.010962,"end_time":"2023-12-30T01:18:04.098769","exception":false,"start_time":"2023-12-30T01:18:04.087807","status":"completed"},"tags":[]},"source":["# Dataset Pre-Processing Task Overview\n","\n","1. Target Variable Label Encoding \n","2. Missing Feature Value Handling \n","3. Feature Scaling \n","4. Categorical Attribute Handling (Should be done at the end since it may change the number of columns)\n","5. Train/Test Split"]},{"cell_type":"markdown","id":"b06f2510","metadata":{"papermill":{"duration":0.010708,"end_time":"2023-12-30T01:18:04.120479","exception":false,"start_time":"2023-12-30T01:18:04.109771","status":"completed"},"tags":[]},"source":["## Target Variable Label Encoding\n","\n","Problem: Since the target column, Status, is a text column, we need to convert the column's values into integers so the machine learning model can process it.\n","\n","Possible Approaches:\n","1. Label Encoding - Convert each text category into an integer label. \n","2. Ordinal Encoding - Convert each text category into an integer label but with a particular order. Used when the categories have some quantitative order that can be taken advantage of like low, medium, high.   \n","3. One-Hot Encoding - Convert each text category into a separate column. For example, this is done in the softmax layer of a neural network. \n","\n","Chosen Approach: Label Encoding - There is no obvious ordering in the 'Status' column and ML libraries typically expect a single target column so we will use label encoding."]},{"cell_type":"code","execution_count":4,"id":"a5ec516b","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:04.145313Z","iopub.status.busy":"2023-12-30T01:18:04.144643Z","iopub.status.idle":"2023-12-30T01:18:04.156233Z","shell.execute_reply":"2023-12-30T01:18:04.154548Z"},"papermill":{"duration":0.028511,"end_time":"2023-12-30T01:18:04.159982","exception":false,"start_time":"2023-12-30T01:18:04.131471","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","C     4965\n","D     2665\n","CL     275\n","Name: count, dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Check to see what the initial text categories are\n","train_df['Status'].value_counts()"]},{"cell_type":"code","execution_count":5,"id":"5486cdc7","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:04.187099Z","iopub.status.busy":"2023-12-30T01:18:04.1863Z","iopub.status.idle":"2023-12-30T01:18:05.744141Z","shell.execute_reply":"2023-12-30T01:18:05.742899Z"},"papermill":{"duration":1.574935,"end_time":"2023-12-30T01:18:05.747172","exception":false,"start_time":"2023-12-30T01:18:04.172237","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["Status\n","0    4965\n","2    2665\n","1     275\n","Name: count, dtype: int64"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Use sklearn's LabelEncoder class to transform the Status column's values from strings to integers.\n","label_encoder = LabelEncoder() \n","train_df['Status'] = label_encoder.fit_transform(train_df['Status'])\n","\n","# Check that the label encoder transformation was applied correctly and the categories are now numbers\n","train_df['Status'].value_counts()"]},{"cell_type":"markdown","id":"4976e9ca","metadata":{"papermill":{"duration":0.011335,"end_time":"2023-12-30T01:18:05.770534","exception":false,"start_time":"2023-12-30T01:18:05.759199","status":"completed"},"tags":[]},"source":["## Missing Feature Value Handling\n","\n","Problem: We need to check if there are any missing feature values since most machine learning algorithms cannot handle an empty cell. For example, a Logistic Regression model would throw an error in training although other algorithms like XGBoost can train normally by substituting in a value as needed. \n","\n","Solution: We check for missing feature values by using the isna DataFrame method which returns a boolean DataFrame where each cell is True if the value is missing and False otherwise. We then apply the sum method to find the number of missing values in each column. "]},{"cell_type":"code","execution_count":6,"id":"9e584b35","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:05.797442Z","iopub.status.busy":"2023-12-30T01:18:05.796987Z","iopub.status.idle":"2023-12-30T01:18:05.811162Z","shell.execute_reply":"2023-12-30T01:18:05.809835Z"},"papermill":{"duration":0.030586,"end_time":"2023-12-30T01:18:05.81353","exception":false,"start_time":"2023-12-30T01:18:05.782944","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column: \n","N_Days           0\n","Drug             0\n","Age              0\n","Sex              0\n","Ascites          0\n","Hepatomegaly     0\n","Spiders          0\n","Edema            0\n","Bilirubin        0\n","Cholesterol      0\n","Albumin          0\n","Copper           0\n","Alk_Phos         0\n","SGOT             0\n","Tryglicerides    0\n","Platelets        0\n","Prothrombin      0\n","Stage            0\n","Status           0\n","dtype: int64\n"]}],"source":["# Check for missing feature values\n","print(\"Number of missing feature values by column: \")\n","print(train_df.isna().sum())"]},{"cell_type":"code","execution_count":7,"id":"1f2beba4","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:05.839879Z","iopub.status.busy":"2023-12-30T01:18:05.839414Z","iopub.status.idle":"2023-12-30T01:18:05.851558Z","shell.execute_reply":"2023-12-30T01:18:05.850079Z"},"papermill":{"duration":0.027674,"end_time":"2023-12-30T01:18:05.853859","exception":false,"start_time":"2023-12-30T01:18:05.826185","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of missing feature values by column in test data: 0\n"]}],"source":["# Alternatively we can check the total number of missing feature values in the dataset this way. \n","print(\"Number of missing feature values by column in test data:\", test_df.isna().sum().sum())"]},{"cell_type":"markdown","id":"284b57e7","metadata":{"papermill":{"duration":0.011044,"end_time":"2023-12-30T01:18:05.8768","exception":false,"start_time":"2023-12-30T01:18:05.865756","status":"completed"},"tags":[]},"source":["## Feature Scaling\n","\n","Problem: Typically since feature column values are combined (often by adding them together) to create the final classification, the ML model will perform better if the features are on the same scale. \n","\n","Possible Feature Scaling Approaches: \n","1. Min-Max Scaling - Scales the data to a fixed range between two values (typically 0 and 1). Most useful for neural networks.\n","2. Standardization (Z-score Normalization) - Scales the data so that the mean is 0 and the standard deviation is 1. Most useful for algorithms that assume a normal distribution of data, such as SVMs and logistic regression.\n","3. Robust Scaling - Scaling based on median and IQR. Most useful for handling significant outliers.\n","4. MaxAbsScaler - Scales each feature based on its maximum absolute value. Useful for sparse data. \n","\n","Chosen Approach: Standardization - Since XGBoost's decision tree classification uses splitting which occurs within a column, different column values do not interact with each other and therefore scaling the features is not necessary. However, since we are using Logistic Regression as our baseline model and features do interact in training (they add together to determine the model's class prediction) we will need to do feature scaling (you don't want one column to be in the millions and the other to be in the single digits). Since we are doing feature scaling specifically for our logistic regression model we will use the Standardization approach. "]},{"cell_type":"code","execution_count":8,"id":"95cb1467","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:05.902717Z","iopub.status.busy":"2023-12-30T01:18:05.901416Z","iopub.status.idle":"2023-12-30T01:18:05.977158Z","shell.execute_reply":"2023-12-30T01:18:05.975741Z"},"papermill":{"duration":0.091876,"end_time":"2023-12-30T01:18:05.980134","exception":false,"start_time":"2023-12-30T01:18:05.888258","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>N_Days</th>\n","      <th>Age</th>\n","      <th>Bilirubin</th>\n","      <th>Cholesterol</th>\n","      <th>Albumin</th>\n","      <th>Copper</th>\n","      <th>Alk_Phos</th>\n","      <th>SGOT</th>\n","      <th>Tryglicerides</th>\n","      <th>Platelets</th>\n","      <th>Prothrombin</th>\n","      <th>Stage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","      <td>5271.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.007790</td>\n","      <td>0.033864</td>\n","      <td>0.001549</td>\n","      <td>0.009851</td>\n","      <td>-0.029617</td>\n","      <td>0.010526</td>\n","      <td>-0.002895</td>\n","      <td>-0.020847</td>\n","      <td>-0.001029</td>\n","      <td>-0.013781</td>\n","      <td>0.004353</td>\n","      <td>0.005175</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.993309</td>\n","      <td>0.973958</td>\n","      <td>1.010406</td>\n","      <td>1.025961</td>\n","      <td>1.025240</td>\n","      <td>1.021709</td>\n","      <td>1.016664</td>\n","      <td>1.003627</td>\n","      <td>1.001441</td>\n","      <td>1.001418</td>\n","      <td>1.014105</td>\n","      <td>0.987967</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-1.817984</td>\n","      <td>-2.384728</td>\n","      <td>-0.601797</td>\n","      <td>-1.180148</td>\n","      <td>-4.588553</td>\n","      <td>-1.052815</td>\n","      <td>-0.802543</td>\n","      <td>-1.808946</td>\n","      <td>-1.567576</td>\n","      <td>-2.323678</td>\n","      <td>-2.084550</td>\n","      <td>-2.345776</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>-0.727654</td>\n","      <td>-0.718300</td>\n","      <td>-0.496885</td>\n","      <td>-0.524971</td>\n","      <td>-0.572940</td>\n","      <td>-0.591648</td>\n","      <td>-0.522026</td>\n","      <td>-0.811772</td>\n","      <td>-0.596648</td>\n","      <td>-0.642910</td>\n","      <td>-0.805263</td>\n","      <td>-1.191649</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>-0.135421</td>\n","      <td>0.117632</td>\n","      <td>-0.391973</td>\n","      <td>-0.263923</td>\n","      <td>0.062625</td>\n","      <td>-0.249068</td>\n","      <td>-0.354452</td>\n","      <td>-0.156896</td>\n","      <td>-0.215892</td>\n","      <td>-0.071221</td>\n","      <td>-0.037691</td>\n","      <td>-0.037522</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.604869</td>\n","      <td>0.627996</td>\n","      <td>0.106359</td>\n","      <td>0.201867</td>\n","      <td>0.640411</td>\n","      <td>0.238452</td>\n","      <td>0.011428</td>\n","      <td>0.478508</td>\n","      <td>0.431393</td>\n","      <td>0.591939</td>\n","      <td>0.474024</td>\n","      <td>1.116605</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2.526884</td>\n","      <td>2.792831</td>\n","      <td>6.663359</td>\n","      <td>7.291089</td>\n","      <td>3.153780</td>\n","      <td>6.642081</td>\n","      <td>6.327728</td>\n","      <td>7.023169</td>\n","      <td>9.188781</td>\n","      <td>3.404652</td>\n","      <td>5.847030</td>\n","      <td>1.116605</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            N_Days          Age    Bilirubin  Cholesterol      Albumin  \\\n","count  5271.000000  5271.000000  5271.000000  5271.000000  5271.000000   \n","mean      0.007790     0.033864     0.001549     0.009851    -0.029617   \n","std       0.993309     0.973958     1.010406     1.025961     1.025240   \n","min      -1.817984    -2.384728    -0.601797    -1.180148    -4.588553   \n","25%      -0.727654    -0.718300    -0.496885    -0.524971    -0.572940   \n","50%      -0.135421     0.117632    -0.391973    -0.263923     0.062625   \n","75%       0.604869     0.627996     0.106359     0.201867     0.640411   \n","max       2.526884     2.792831     6.663359     7.291089     3.153780   \n","\n","            Copper     Alk_Phos         SGOT  Tryglicerides    Platelets  \\\n","count  5271.000000  5271.000000  5271.000000    5271.000000  5271.000000   \n","mean      0.010526    -0.002895    -0.020847      -0.001029    -0.013781   \n","std       1.021709     1.016664     1.003627       1.001441     1.001418   \n","min      -1.052815    -0.802543    -1.808946      -1.567576    -2.323678   \n","25%      -0.591648    -0.522026    -0.811772      -0.596648    -0.642910   \n","50%      -0.249068    -0.354452    -0.156896      -0.215892    -0.071221   \n","75%       0.238452     0.011428     0.478508       0.431393     0.591939   \n","max       6.642081     6.327728     7.023169       9.188781     3.404652   \n","\n","       Prothrombin        Stage  \n","count  5271.000000  5271.000000  \n","mean      0.004353     0.005175  \n","std       1.014105     0.987967  \n","min      -2.084550    -2.345776  \n","25%      -0.805263    -1.191649  \n","50%      -0.037691    -0.037522  \n","75%       0.474024     1.116605  \n","max       5.847030     1.116605  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.preprocessing import StandardScaler \n","\n","numerical_cols = train_df.columns.difference(categorical_cols)\n","\n","std_scaler = StandardScaler() \n","std_scaler.fit(train_df[numerical_cols])\n","\n","train_df[numerical_cols] = std_scaler.transform(train_df[numerical_cols])\n","test_df[numerical_cols] = std_scaler.transform(test_df[numerical_cols])\n","                      \n","# Confirm the transformation was successful by seeing if the mean = 0 and std = 1 for numerical columns\n","test_df.describe()"]},{"cell_type":"markdown","id":"9d35dc01","metadata":{"papermill":{"duration":0.011627,"end_time":"2023-12-30T01:18:06.00451","exception":false,"start_time":"2023-12-30T01:18:05.992883","status":"completed"},"tags":[]},"source":["## Handling Categorical Attributes/Columns \n","Problem: We use the DataFrame info method to find that there are six categorical columns: drug, sex, ascites, hepatomegaly, spiders, edema which we need to convert from text into numerical values since machine learning models cannot handle text data naturally, they can only handle numbers.\n","\n","Possible Approaches: The main approaches for categorical attribute handling are \n","1. Ordinal Encoding - Useful when the categories correspond to an ascending or descending order. \n","2. One-Hot Encoding - For each categorical column, convert it into multiple columns, one for each possible category. This is used when the categories do not have an obvious logical order. \n","3. Numerical Feature Replacement (Advanced) - In cases where the number of categories is cery large (hundreds or thousands) one should consider replacing the categorical columns with a numerical column that converts each category into some number. For example, one could convert a country code into the country's population. \n","4. Embedding Replacement (Advanced) - Alternatively, one can replace categories with embeddings, which are low dimensional vectors that represent the category. \n","\n","Chosen Approach : One-Hot Encoding - In this case, we use a one-hot encoding since none of the categories seem to have an order to the classes (ruling out ordinal encoding) and the number of categories for each column is low (under 10 for all categorical columns) which rules out needing advanced methods. "]},{"cell_type":"code","execution_count":9,"id":"af3f19c7","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:06.031712Z","iopub.status.busy":"2023-12-30T01:18:06.031272Z","iopub.status.idle":"2023-12-30T01:18:06.049166Z","shell.execute_reply":"2023-12-30T01:18:06.047688Z"},"papermill":{"duration":0.034877,"end_time":"2023-12-30T01:18:06.051868","exception":false,"start_time":"2023-12-30T01:18:06.016991","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["N_Days           461\n","Drug               2\n","Age              391\n","Sex                2\n","Ascites            2\n","Hepatomegaly       2\n","Spiders            2\n","Edema              3\n","Bilirubin        111\n","Cholesterol      226\n","Albumin          160\n","Copper           171\n","Alk_Phos         364\n","SGOT             206\n","Tryglicerides    154\n","Platelets        227\n","Prothrombin       49\n","Stage              4\n","Status             3\n","dtype: int64\n"]}],"source":["# Confirm that the number of categories in the categorical columns is manageable (< 100)\n","unique_values_per_column = train_df.nunique()\n","print(unique_values_per_column)"]},{"cell_type":"code","execution_count":10,"id":"02609d54","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:06.079196Z","iopub.status.busy":"2023-12-30T01:18:06.078765Z","iopub.status.idle":"2023-12-30T01:18:06.122605Z","shell.execute_reply":"2023-12-30T01:18:06.121243Z"},"papermill":{"duration":0.060615,"end_time":"2023-12-30T01:18:06.125281","exception":false,"start_time":"2023-12-30T01:18:06.064666","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 5271 entries, 0 to 5270\n","Data columns (total 25 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   N_Days                5271 non-null   float64\n"," 1   Age                   5271 non-null   float64\n"," 2   Bilirubin             5271 non-null   float64\n"," 3   Cholesterol           5271 non-null   float64\n"," 4   Albumin               5271 non-null   float64\n"," 5   Copper                5271 non-null   float64\n"," 6   Alk_Phos              5271 non-null   float64\n"," 7   SGOT                  5271 non-null   float64\n"," 8   Tryglicerides         5271 non-null   float64\n"," 9   Platelets             5271 non-null   float64\n"," 10  Prothrombin           5271 non-null   float64\n"," 11  Stage                 5271 non-null   float64\n"," 12  Drug_D-penicillamine  5271 non-null   bool   \n"," 13  Drug_Placebo          5271 non-null   bool   \n"," 14  Sex_F                 5271 non-null   bool   \n"," 15  Sex_M                 5271 non-null   bool   \n"," 16  Ascites_N             5271 non-null   bool   \n"," 17  Ascites_Y             5271 non-null   bool   \n"," 18  Hepatomegaly_N        5271 non-null   bool   \n"," 19  Hepatomegaly_Y        5271 non-null   bool   \n"," 20  Spiders_N             5271 non-null   bool   \n"," 21  Spiders_Y             5271 non-null   bool   \n"," 22  Edema_N               5271 non-null   bool   \n"," 23  Edema_S               5271 non-null   bool   \n"," 24  Edema_Y               5271 non-null   bool   \n","dtypes: bool(13), float64(12)\n","memory usage: 561.2 KB\n"]}],"source":["# Convert the categorical columns into one-hot encodings\n","status = train_df['Status']\n","train_df_dummies = pd.get_dummies(train_df.drop('Status', axis=1))\n","train_df = pd.concat([train_df_dummies, status], axis=1)\n","test_df = pd.get_dummies(test_df)\n","\n","# Confirm the transformation was successful\n","test_df.info()"]},{"cell_type":"markdown","id":"10dfa515","metadata":{"papermill":{"duration":0.011971,"end_time":"2023-12-30T01:18:06.149721","exception":false,"start_time":"2023-12-30T01:18:06.13775","status":"completed"},"tags":[]},"source":["## Train/Test Split\n","\n","Problem: We need to split the dataset into a training dataset and a testing/tuning dataset.\n","\n","Possible Approaches\n","1. K-Fold Cross-Validation - In this approach the dataset is divided into k equal-sized subsets. Then we train the model k times, each time using a different subset as the validation set and the other k-1 subsets as the training set. Finally we use the average score among all k models as the final score. This approach is commonly used since it essentially utilizes  more of the training data for validation (90% vs. 80%). Typical values range are k=5 or k=10. \n","2. Train/Validation/Test Split - In this approach we split the dataset into three distinct sets: a training set for training the model, a validation set for tuning the hyperparameters and a test set for evaluating the final model. By separating the test set and validation set we reduce / avoid overfitting to the test set. A typical split varies but could be 80/10/10 or 70/15/15.\n","\n","Chosen Approach : Initially we used the train/validation/test approach since it is simpler to implement. However, we decided that implementing that impementing k-fold cross validation was not too much harder and would utilize more of the data for training / tuning so eventually we settled on this approach with k=10. "]},{"cell_type":"code","execution_count":11,"id":"bd0b320b","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:06.177159Z","iopub.status.busy":"2023-12-30T01:18:06.176755Z","iopub.status.idle":"2023-12-30T01:18:06.30797Z","shell.execute_reply":"2023-12-30T01:18:06.306707Z"},"papermill":{"duration":0.148281,"end_time":"2023-12-30T01:18:06.310849","exception":false,"start_time":"2023-12-30T01:18:06.162568","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Set parameters for k-folds cross-validation\n","kfold = StratifiedKFold(n_splits=5, shuffle=True)\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Split the training set into a training and test set \n","X = train_df.drop(\"Status\", axis=1)\n","y = train_df[\"Status\"]\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y,  test_size = 0.2)"]},{"cell_type":"markdown","id":"93c30a37","metadata":{"papermill":{"duration":0.012188,"end_time":"2023-12-30T01:18:06.335351","exception":false,"start_time":"2023-12-30T01:18:06.323163","status":"completed"},"tags":[]},"source":["# Logistic Regression Baseline Model"]},{"cell_type":"code","execution_count":12,"id":"9d68a64b","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:06.364512Z","iopub.status.busy":"2023-12-30T01:18:06.364033Z","iopub.status.idle":"2023-12-30T01:18:06.964476Z","shell.execute_reply":"2023-12-30T01:18:06.962844Z"},"papermill":{"duration":0.620182,"end_time":"2023-12-30T01:18:06.969513","exception":false,"start_time":"2023-12-30T01:18:06.349331","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[9.02992366e-01 1.05900228e-02 8.64176112e-02]\n"," [4.49692868e-01 7.06604274e-02 4.79646705e-01]\n"," [7.00271409e-01 5.75837394e-02 2.42144852e-01]\n"," ...\n"," [7.61899198e-01 4.46811067e-02 1.93419696e-01]\n"," [5.34713391e-04 3.44625616e-03 9.96019030e-01]\n"," [2.19403140e-01 2.81589678e-02 7.52437892e-01]] \n","\n","Validation Set Log Loss: 0.5345424812587198\n"]}],"source":["from sklearn.linear_model import LogisticRegression \n","from sklearn.metrics import log_loss\n","\n","# Train the model using sci-kit learn's Logistic Regression model \n","model = LogisticRegression(max_iter = 1000)\n","# The fit method learns the parameters (weights) for the model\n","model.fit(X_train, y_train)\n","# Predict the probability of each class for the validation set and the test set \n","y_val_pred_lr = model.predict_proba(X_val)\n","y_pred_lr = model.predict_proba(test_df)\n","print(y_val_pred_lr, \"\\n\")\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_lr)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"dd8f474b","metadata":{"papermill":{"duration":0.025713,"end_time":"2023-12-30T01:18:07.02206","exception":false,"start_time":"2023-12-30T01:18:06.996347","status":"completed"},"tags":[]},"source":["# Random Forest Model "]},{"cell_type":"code","execution_count":13,"id":"735d3aff","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:07.07397Z","iopub.status.busy":"2023-12-30T01:18:07.073579Z","iopub.status.idle":"2023-12-30T01:18:16.754369Z","shell.execute_reply":"2023-12-30T01:18:16.753065Z"},"papermill":{"duration":9.710009,"end_time":"2023-12-30T01:18:16.757757","exception":false,"start_time":"2023-12-30T01:18:07.047748","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[9.57301210e-01 1.27254690e-03 4.14262427e-02]\n"," [6.29228569e-01 4.74308603e-02 3.23340570e-01]\n"," [6.61118158e-01 3.40294719e-02 3.04852370e-01]\n"," ...\n"," [8.28900117e-01 2.54099201e-02 1.45689963e-01]\n"," [5.50251138e-03 5.27777778e-04 9.93969711e-01]\n"," [4.03747756e-01 4.42960697e-02 5.51956174e-01]] \n","\n","Validation Set Log Loss: 0.45299829287372545\n"]}],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Train the model using sci-kit learn's Random Forest model \n","rf_classifier = RandomForestClassifier(n_estimators=600, max_depth=30, min_samples_split=10) \n","# The fit method learns the parameters (weights) for the model\n","rf_classifier.fit(X_train, y_train) \n","# Predict the probability of each class for the validation set and the test set (Kaggle's unlabeled dataset)\n","y_val_pred_rf = rf_classifier.predict_proba(X_val)\n","y_pred_rf = rf_classifier.predict_proba(test_df)\n","print(y_val_pred_rf, \"\\n\")\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_rf)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"89934f7d","metadata":{"papermill":{"duration":0.013662,"end_time":"2023-12-30T01:18:16.784552","exception":false,"start_time":"2023-12-30T01:18:16.77089","status":"completed"},"tags":[]},"source":["# XGBoost Model"]},{"cell_type":"code","execution_count":14,"id":"7535e42f","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:16.81182Z","iopub.status.busy":"2023-12-30T01:18:16.811377Z","iopub.status.idle":"2023-12-30T01:18:19.760912Z","shell.execute_reply":"2023-12-30T01:18:19.759775Z"},"papermill":{"duration":2.966098,"end_time":"2023-12-30T01:18:19.763516","exception":false,"start_time":"2023-12-30T01:18:16.797418","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.95512545 0.00346334 0.04141126]\n"," [0.7542091  0.02001405 0.22577679]\n"," [0.6024575  0.06493251 0.33260995]\n"," ...\n"," [0.89321953 0.00345223 0.10332823]\n"," [0.00363269 0.00197448 0.99439275]\n"," [0.43488002 0.02149551 0.54362446]] \n","\n","Validation Set Log Loss: 0.4122643698079633\n"]}],"source":["from xgboost import XGBClassifier \n","\n","# Train the model using xgboost's XGBClassifier model \n","xgb_model = XGBClassifier(n_estimators=607, max_depth=6, learning_rate=0.0419, colsample_bytree=0.168, min_child_weight=17, subsample=0.7) \n","# The fit method learns the parameters (weights) for the model\n","xgb_model.fit(X_train, y_train)\n","# Predict the probability of each class for the validation set and the test set (Kaggle's unlabeled dataset)\n","y_val_pred_xgb = xgb_model.predict_proba(X_val)\n","y_pred_xgb = xgb_model.predict_proba(test_df)\n","print(y_val_pred_xgb, \"\\n\")\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_xgb)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"code","execution_count":15,"id":"5c9cfdd3","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:19.791498Z","iopub.status.busy":"2023-12-30T01:18:19.791056Z","iopub.status.idle":"2023-12-30T01:18:19.799044Z","shell.execute_reply":"2023-12-30T01:18:19.798125Z"},"papermill":{"duration":0.024414,"end_time":"2023-12-30T01:18:19.801345","exception":false,"start_time":"2023-12-30T01:18:19.776931","status":"completed"},"tags":[]},"outputs":[],"source":["# Define a function for doing GridSearch/RandomizedSearchCV for hyperparameter tuning \n","\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","\n","def grid_search(X, y, param_grid, random=False): \n","    xgb = XGBClassifier() \n","    \n","    if random: \n","        grid = RandomizedSearchCV(xgb, params, cv=kfold, n_iter = 20, n_jobs=-1)\n","        \n","    else :\n","        grid = GridSearchCV(xgb, param_grid=param_grid, cv=kfold, scoring='neg_log_loss', n_jobs=-1)\n","        \n","    grid.fit(X,y) \n","    \n","    best_params = grid.best_params_\n","    print(\"Best params:\", best_params)\n","    \n","    best_score = grid.best_score_\n","    print(\"Training score: {:.3f}\".format(best_score))"]},{"cell_type":"code","execution_count":16,"id":"f09f2c7d","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:19.830165Z","iopub.status.busy":"2023-12-30T01:18:19.829079Z","iopub.status.idle":"2023-12-30T01:18:36.102562Z","shell.execute_reply":"2023-12-30T01:18:36.101161Z"},"papermill":{"duration":16.291049,"end_time":"2023-12-30T01:18:36.105772","exception":false,"start_time":"2023-12-30T01:18:19.814723","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Best params: {'learning_rate': 0.0419, 'max_depth': 6, 'min_child_weight': 17, 'n_estimators': 607}\n","Training score: -0.470\n"]}],"source":["from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","\n","# Run this cell once for each hyperparameter, finding the best value for that hyperparameter\n","grid_search(X_train, y_train, param_grid = {\n","    'n_estimators': [607],\n","    'learning_rate': [0.0419],\n","    'max_depth': [6],\n","    'min_child_weight': [17]})"]},{"cell_type":"markdown","id":"a1cdb820","metadata":{"papermill":{"duration":0.014627,"end_time":"2023-12-30T01:18:36.134331","exception":false,"start_time":"2023-12-30T01:18:36.119704","status":"completed"},"tags":[]},"source":["## LGBM Classifier"]},{"cell_type":"code","execution_count":17,"id":"d2a5c6b6","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:36.165562Z","iopub.status.busy":"2023-12-30T01:18:36.165077Z","iopub.status.idle":"2023-12-30T01:18:46.295071Z","shell.execute_reply":"2023-12-30T01:18:46.293862Z"},"papermill":{"duration":10.148974,"end_time":"2023-12-30T01:18:46.298106","exception":false,"start_time":"2023-12-30T01:18:36.149132","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Set Log Loss: 0.4111899230141859\n"]}],"source":["import lightgbm as lgb\n","\n","# Define your parameters\n","lgb_params = {\n","    'max_depth': 15,\n","    'min_child_samples': 13,\n","    'learning_rate': 0.05285597081335651,\n","    'n_estimators': 294,\n","    'min_child_weight': 5,\n","    'colsample_bytree': 0.10012816493265511,\n","    'reg_alpha': 0.8767668608061822,\n","    'reg_lambda': 0.8705834466355764\n","}\n","\n","# Create the LGBMClassifier with the specified parameters\n","lgbm_model = lgb.LGBMClassifier(**lgb_params)\n","\n","# Now you can fit this classifier to your data\n","lgbm_model.fit(X_train, y_train)\n","\n","# And make predictions\n","y_val_pred_lgbm = lgbm_model.predict_proba(X_val)\n","y_pred_lgbm = lgbm_model.predict_proba(test_df)\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_lgbm)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"60767569","metadata":{"papermill":{"duration":0.013032,"end_time":"2023-12-30T01:18:46.324825","exception":false,"start_time":"2023-12-30T01:18:46.311793","status":"completed"},"tags":[]},"source":["# Ensemble Model"]},{"cell_type":"code","execution_count":18,"id":"432a5e67","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:18:46.424469Z","iopub.status.busy":"2023-12-30T01:18:46.423467Z","iopub.status.idle":"2023-12-30T01:19:07.964145Z","shell.execute_reply":"2023-12-30T01:19:07.962195Z"},"papermill":{"duration":21.627876,"end_time":"2023-12-30T01:19:07.966811","exception":false,"start_time":"2023-12-30T01:18:46.338935","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Set Log Loss: 0.41335557452604627\n"]}],"source":["from sklearn.ensemble import VotingClassifier\n","\n","# Train an Ensemble model using a combination of the Random Forest, XGBoost, and LGBM Classifiers\n","\n","ensemble_model = VotingClassifier(\n","    estimators=[\n","        ('lgb', lgbm_model),\n","        ('xgb', xgb_model),\n","        ('rf', rf_classifier),\n","    ],\n","    voting='soft'\n",")\n","\n","ensemble_model.fit(X_train, y_train)\n","\n","# Make predictions using the ensemble\n","y_val_pred_ensemble = ensemble_model.predict_proba(X_val)\n","y_pred_ensemble = ensemble_model.predict_proba(test_df)\n","\n","# Evaluate the model predictions by calculating log loss for the validation set\n","loss = log_loss(y_val, y_val_pred_ensemble)\n","print(f'Validation Set Log Loss: {loss}')"]},{"cell_type":"markdown","id":"cfd4164e","metadata":{"papermill":{"duration":0.012956,"end_time":"2023-12-30T01:19:07.992953","exception":false,"start_time":"2023-12-30T01:19:07.979997","status":"completed"},"tags":[]},"source":["# Kaggle Submission Processing\n","1. Create a submission dataframe from the model's predictions \n","2. Concatenate the data id column values to adhere to submission formatting requirements\n","3. Convert the submission dataframe into a csv file for submission \n","4. Now in order to submit to Kaggle, save the notebook and navigate to the Submissions page for this competition and click 'Submit Prediction' in the top-right corner -> Notebook -> Submit. "]},{"cell_type":"code","execution_count":19,"id":"7e173970","metadata":{"execution":{"iopub.execute_input":"2023-12-30T01:19:08.021508Z","iopub.status.busy":"2023-12-30T01:19:08.020814Z","iopub.status.idle":"2023-12-30T01:19:08.079833Z","shell.execute_reply":"2023-12-30T01:19:08.07827Z"},"papermill":{"duration":0.076846,"end_time":"2023-12-30T01:19:08.082677","exception":false,"start_time":"2023-12-30T01:19:08.005831","status":"completed"},"tags":[]},"outputs":[],"source":["# Modify the probability predictions into the submission format \n","submission_df = pd.DataFrame(y_pred_ensemble, columns=['Status_C', 'Status_CL', 'Status_D'])\n","final_submission_df = pd.concat([test_id_df, submission_df], axis=1)\n","final_submission_df.head(10)\n","\n","# Create a submission.csv file that Kaggle will automatically evaluate for submission\n","final_submission_df.to_csv('submission.csv', index = False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7000181,"sourceId":60893,"sourceType":"competition"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":73.138525,"end_time":"2023-12-30T01:19:10.717566","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-12-30T01:17:57.579041","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}